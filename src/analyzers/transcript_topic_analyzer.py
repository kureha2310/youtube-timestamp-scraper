#!/usr/bin/env python3
"""
YouTubeå­—å¹•ã‹ã‚‰è©±é¡Œã‚’è‡ªå‹•æŠ½å‡ºã™ã‚‹ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼
"""

import re
import json
from typing import List, Dict, Tuple, Optional
from datetime import timedelta
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
from dataclasses import dataclass

@dataclass
class TopicSegment:
    """è©±é¡Œã‚»ã‚°ãƒ¡ãƒ³ãƒˆ"""
    start_time: float  # é–‹å§‹æ™‚é–“ï¼ˆç§’ï¼‰
    end_time: float    # çµ‚äº†æ™‚é–“ï¼ˆç§’ï¼‰
    topic: str         # è©±é¡Œ
    keywords: List[str]  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    confidence: float  # ä¿¡é ¼åº¦
    
    @property
    def duration(self) -> float:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é•·ã•ï¼ˆç§’ï¼‰"""
        return self.end_time - self.start_time
    
    @property
    def start_timestamp(self) -> str:
        """é–‹å§‹æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼"""
        return self._seconds_to_timestamp(self.start_time)
    
    @property
    def end_timestamp(self) -> str:
        """çµ‚äº†æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼"""
        return self._seconds_to_timestamp(self.end_time)
    
    @property
    def youtube_link(self) -> str:
        """YouTube ãƒªãƒ³ã‚¯ï¼ˆé–‹å§‹æ™‚é–“ä»˜ãï¼‰"""
        return f"&t={int(self.start_time)}"
    
    def _seconds_to_timestamp(self, seconds: float) -> str:
        """ç§’æ•°ã‚’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        hours = minutes // 60
        minutes = minutes % 60
        
        if hours > 0:
            return f"{hours}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes}:{secs:02d}"

class TranscriptTopicAnalyzer:
    def __init__(self):
        """åˆæœŸåŒ–"""
        # è©±é¡Œè»¢æ›ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.topic_transition_keywords = [
            # æ˜ç¢ºãªè©±é¡Œè»¢æ›
            "ã•ã¦", "ãã‚Œã§ã¯", "æ¬¡ã«", "ç¶šã„ã¦", "ã¨ã“ã‚ã§", "ãã†ã„ãˆã°",
            "è©±ã¯å¤‰ã‚ã£ã¦", "è©±å¤‰ã‚ã‚‹ã‘ã©", "åˆ¥ã®è©±", "ãã†ãã†",
            
            # æ™‚é–“çš„ãªåŒºåˆ‡ã‚Š
            "ä»Šåº¦ã¯", "ä»Šå›ã¯", "æœ€åˆã«", "æœ€å¾Œã«", "çµ‚ã‚ã‚Šã«",
            
            # è³ªå•ãƒ»å¿œç­”
            "è³ªå•", "èããŸã„", "ç­”ãˆ", "å›ç­”", "ã‚³ãƒ¡ãƒ³ãƒˆ",
            
            # ã‚²ãƒ¼ãƒ é…ä¿¡ç‰¹æœ‰
            "ã‚²ãƒ¼ãƒ ", "ãƒ—ãƒ¬ã‚¤", "æ”»ç•¥", "ãƒ¬ãƒ™ãƒ«", "ã‚¹ãƒ†ãƒ¼ã‚¸", "ãƒœã‚¹",
            
            # æ­Œé…ä¿¡ç‰¹æœ‰
            "æ­Œ", "ã†ãŸ", "æ›²", "ãƒªã‚¯ã‚¨ã‚¹ãƒˆ", "æ¬¡ã®", "1æ›²ç›®", "2æ›²ç›®",
            
            # é›‘è«‡ç‰¹æœ‰
            "æœ€è¿‘", "ä»Šæ—¥", "æ˜¨æ—¥", "æ˜æ—¥", "ä»Šåº¦", "å‰å›",
        ]
        
        # å¼·ã„è©±é¡Œè»¢æ›ï¼ˆã‚ˆã‚Šç¢ºå®ŸãªåŒºåˆ‡ã‚Šï¼‰
        self.strong_transition_patterns = [
            r"ãã‚Œã§ã¯.*?(?:è¡Œ|ã„)ãã¾ã™",
            r"æ¬¡.*?(?:è¡Œ|ã„)ãã¾ã™",
            r"(?:ã•ã¦|ã§ã¯).*?(?:å§‹|ã¯ã˜)ã‚",
            r"(?:ä»Šåº¦|ä»Šå›).*?(?:ã‚„ã‚Š|ã™ã‚‹)",
            r"(?:æœ€åˆ|ã¾ãš).*?(?:ã‹ã‚‰|ã¯)",
            r"(?:ç¶šã„|ã¤ã¥ã„)ã¦.*?(?:ã¯|ã‚’)",
            r"(?:è©±.*?å¤‰ã‚|åˆ¥.*?è©±)",
            r"(?:è³ªå•|ã‚³ãƒ¡ãƒ³ãƒˆ).*?(?:æ¥|ã)ã¦",
            r"\d+(?:æ›²ç›®|ç•ªç›®|ã¤ç›®)",
        ]
        
        # ãƒˆãƒ”ãƒƒã‚¯æ¨å®šç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸
        self.topic_keywords = {
            "ã‚²ãƒ¼ãƒ ": ["ã‚²ãƒ¼ãƒ ", "ãƒ—ãƒ¬ã‚¤", "æ”»ç•¥", "ãƒ¬ãƒ™ãƒ«", "ã‚¹ãƒ†ãƒ¼ã‚¸", "ãƒœã‚¹", "ã‚­ãƒ£ãƒ©", "ã‚¢ã‚¤ãƒ†ãƒ ", "RPG", "FPS"],
            "æ­Œãƒ»éŸ³æ¥½": ["æ­Œ", "ã†ãŸ", "æ›²", "éŸ³æ¥½", "æ­Œè©", "ãƒ¡ãƒ­ãƒ‡ã‚£", "ãƒªã‚ºãƒ ", "ãƒœãƒ¼ã‚«ãƒ«", "æ¥½å™¨", "ä½œè©", "ä½œæ›²"],
            "é›‘è«‡": ["æœ€è¿‘", "ä»Šæ—¥", "æ˜¨æ—¥", "æ—¥å¸¸", "ç”Ÿæ´»", "æ€ã£ãŸ", "æ„Ÿã˜ãŸ", "è©±", "ãŠã—ã‚ƒã¹ã‚Š"],
            "è³ªå•å›ç­”": ["è³ªå•", "ã‚³ãƒ¡ãƒ³ãƒˆ", "ç­”ãˆ", "å›ç­”", "èã‹ã‚Œ", "æ•™ãˆ", "èª¬æ˜"],
            "æ–™ç†": ["æ–™ç†", "é£Ÿã¹", "é£²ã¿", "ãƒ¬ã‚·ãƒ”", "ä½œã‚Š", "å‘³", "ç¾å‘³ã—ã„", "é£Ÿæ"],
            "ãŠçŸ¥ã‚‰ã›": ["ãŠçŸ¥ã‚‰ã›", "å‘ŠçŸ¥", "äºˆå®š", "ã‚¤ãƒ™ãƒ³ãƒˆ", "é…ä¿¡", "å‹•ç”»", "ã‚³ãƒ©ãƒœ", "ä¼ç”»"],
            "æ„Ÿæƒ³ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼": ["æ„Ÿæƒ³", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "è©•ä¾¡", "è‰¯ã‹ã£ãŸ", "é¢ç™½ã„", "ã¤ã¾ã‚‰ãªã„", "ã™ã”ã„"],
        }
        
        # ç„¡è¦–ã™ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆãƒã‚¤ã‚ºï¼‰
        self.ignore_patterns = [
            r"^[ã‚ãƒ¼]+$",  # ã€Œã‚ãƒ¼ã€ã ã‘
            r"^[ãˆãƒ¼]+$",  # ã€Œãˆãƒ¼ã€ã ã‘
            r"^[ã†ãƒ¼]+$",  # ã€Œã†ãƒ¼ã€ã ã‘
            r"^[ã‚“ãƒ¼]+$",  # ã€Œã‚“ãƒ¼ã€ã ã‘
            r"^[ã¯ã„]+$",  # ã€Œã¯ã„ã€ã ã‘
            r"^[ãã†]+$",  # ã€Œãã†ã€ã ã‘
            r"^w+$",       # ã€Œwã€ã ã‘
            r"^\.+$",      # ãƒ”ãƒªã‚ªãƒ‰ã ã‘
            r"^[ï¼Ÿï¼ã€‚ã€]+$",  # è¨˜å·ã ã‘
        ]
    
    def get_transcript(self, video_id: str, language: str = 'ja') -> List[Dict]:
        """YouTubeå‹•ç”»ã®å­—å¹•ã‚’å–å¾—"""
        try:
            # åˆ©ç”¨å¯èƒ½ãªå­—å¹•ã‚’ãƒªã‚¹ãƒˆè¡¨ç¤º
            try:
                transcript_list = YouTubeTranscriptApi().list(video_id)
                print(f"åˆ©ç”¨å¯èƒ½ãªå­—å¹•: {len(transcript_list)}ç¨®é¡")
                
                # åˆ©ç”¨å¯èƒ½ãªå­—å¹•ã®è¨€èªã‚’è¡¨ç¤º
                languages = []
                for transcript in transcript_list:
                    languages.append(transcript.get('language_code', 'unknown'))
                print(f"è¨€èª: {', '.join(languages)}")
                
            except Exception as e:
                print(f"å­—å¹•ãƒªã‚¹ãƒˆå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã¾ãšæ—¥æœ¬èªã®å­—å¹•ã‚’ç›´æ¥å–å¾—ã‚’è©¦ã™
            try:
                transcript_data = YouTubeTranscriptApi().fetch(video_id, languages=['ja'])
                print(f"æ—¥æœ¬èªå­—å¹•ã‚’å–å¾—ã—ã¾ã—ãŸ: {len(transcript_data)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
                return transcript_data
            except Exception as e:
                print(f"æ—¥æœ¬èªå­—å¹•å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            
            # è‹±èªå­—å¹•ã‚’è©¦ã™
            try:
                transcript_data = YouTubeTranscriptApi().fetch(video_id, languages=['en'])
                print(f"è‹±èªå­—å¹•ã‚’å–å¾—ã—ã¾ã—ãŸ: {len(transcript_data)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
                return transcript_data
            except Exception as e:
                print(f"è‹±èªå­—å¹•å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            
            # è‡ªå‹•ç”Ÿæˆå­—å¹•ã‚’è©¦ã™ï¼ˆè¨€èªæŒ‡å®šãªã—ï¼‰
            try:
                transcript_data = YouTubeTranscriptApi().fetch(video_id)
                print(f"è‡ªå‹•ç”Ÿæˆå­—å¹•ã‚’å–å¾—ã—ã¾ã—ãŸ: {len(transcript_data)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
                return transcript_data
            except Exception as e:
                print(f"è‡ªå‹•ç”Ÿæˆå­—å¹•å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            
            return []
                
        except Exception as e:
            print(f"å­—å¹•å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # HTMLã‚¿ã‚°ã‚’é™¤å»
        text = re.sub(r'<[^>]*>', '', text)
        
        # ç‰¹æ®Šæ–‡å­—ã‚’æ­£è¦åŒ–
        text = re.sub(r'[â™ªâ™«â™¬ğŸµğŸ¶]', '', text)  # éŸ³æ¥½è¨˜å·ã‚’é™¤å»
        text = re.sub(r'\s+', ' ', text)  # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
        
        return text.strip()
    
    def is_topic_transition(self, text: str, prev_text: str = "") -> Tuple[bool, float]:
        """è©±é¡Œè»¢æ›ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        confidence = 0.0
        
        # å¼·ã„è»¢æ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        for pattern in self.strong_transition_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                confidence += 0.8
        
        # è»¢æ›ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        for keyword in self.topic_transition_keywords:
            if keyword in text:
                confidence += 0.3
        
        # æ–‡ã®å¢ƒç•Œã‚’è€ƒæ…®
        if re.search(r'[ã€‚ï¼ï¼Ÿ].*?(?:ã•ã¦|ãã‚Œã§ã¯|æ¬¡ã«|ç¶šã„ã¦)', text):
            confidence += 0.5
        
        # å‰ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ã®é–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        if prev_text:
            # å…¨ãé•ã†å†…å®¹ã®å ´åˆã¯è»¢æ›ã®å¯èƒ½æ€§
            common_words = set(text.split()) & set(prev_text.split())
            if len(common_words) < 2:
                confidence += 0.2
        
        return confidence > 0.5, min(confidence, 1.0)
    
    def extract_keywords(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # åè©ã¨å‹•è©ã‚’ä¸­å¿ƒã«æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        keywords = []
        
        # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆæ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ï¼‰
        word_patterns = [
            r'[ã‚²ãƒ¼ãƒ é…ä¿¡å®Ÿæ³ãƒ—ãƒ¬ã‚¤]+',
            r'[æ­Œã†ãŸéŸ³æ¥½æ›²å£°]+',
            r'[æ–™ç†é£Ÿäº‹é£¯ãƒ¬ã‚·ãƒ”]+',
            r'[è³ªå•å›ç­”ã‚³ãƒ¡ãƒ³ãƒˆ]+',
            r'[å‘ŠçŸ¥ãŠçŸ¥ã‚‰ã›äºˆå®šä¼ç”»]+',
        ]
        
        for pattern in word_patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches)
        
        # é »å‡ºã™ã‚‹åè©ã£ã½ã„éƒ¨åˆ†ã‚’æŠ½å‡º
        noun_pattern = r'[ã‚¡-ãƒ¶ãƒ¼]+|[ã-ã‚“]+|[ä¸€-é¾¯]+'
        potential_nouns = re.findall(noun_pattern, text)
        
        # é•·ã•ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        keywords.extend([word for word in potential_nouns if 2 <= len(word) <= 6])
        
        return list(set(keywords))[:5]  # é‡è¤‡é™¤å»ã—ã¦æœ€å¤§5å€‹
    
    def classify_topic(self, text: str, keywords: List[str]) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰è©±é¡Œã‚’åˆ†é¡"""
        topic_scores = {}
        
        for topic, topic_keywords in self.topic_keywords.items():
            score = 0
            combined_text = text + " " + " ".join(keywords)
            
            for keyword in topic_keywords:
                if keyword in combined_text:
                    score += 1
            
            if score > 0:
                topic_scores[topic] = score
        
        if topic_scores:
            return max(topic_scores, key=topic_scores.get)
        else:
            return "ãã®ä»–"
    
    def should_ignore_segment(self, text: str) -> bool:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç„¡è¦–ã™ã¹ãã‹ã©ã†ã‹åˆ¤å®š"""
        for pattern in self.ignore_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                return True
        
        # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        if len(text.strip()) < 3:
            return True
        
        # è¨˜å·ã‚„æ•°å­—ã®ã¿
        if re.match(r'^[0-9\s\W]+$', text):
            return True
        
        return False
    
    def merge_short_segments(self, segments: List[TopicSegment], min_duration: float = 30.0) -> List[TopicSegment]:
        """çŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‰å¾Œã¨çµ±åˆ"""
        if not segments:
            return segments
        
        merged = []
        current = segments[0]
        
        for next_segment in segments[1:]:
            # ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒçŸ­ã„å ´åˆã¯æ¬¡ã¨çµ±åˆ
            if current.duration < min_duration:
                # ãƒˆãƒ”ãƒƒã‚¯ãŒåŒã˜ã‹ã€ã‚ˆã‚Šé•·ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒˆãƒ”ãƒƒã‚¯ã‚’æ¡ç”¨
                if current.topic == next_segment.topic or next_segment.duration > current.duration:
                    topic = next_segment.topic
                else:
                    topic = current.topic
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµ±åˆ
                combined_keywords = list(set(current.keywords + next_segment.keywords))[:5]
                
                # çµ±åˆã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
                current = TopicSegment(
                    start_time=current.start_time,
                    end_time=next_segment.end_time,
                    topic=topic,
                    keywords=combined_keywords,
                    confidence=(current.confidence + next_segment.confidence) / 2
                )
            else:
                # ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç¢ºå®šã—ã€æ¬¡ã«é€²ã‚€
                merged.append(current)
                current = next_segment
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
        merged.append(current)
        
        return merged
    
    def analyze_topics(self, video_id: str, min_segment_duration: float = 30.0) -> List[TopicSegment]:
        """å­—å¹•ã‹ã‚‰è©±é¡Œã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åˆ†æ"""
        print(f"å‹•ç”» {video_id} ã®å­—å¹•ã‚’åˆ†æä¸­...")
        
        # å­—å¹•ã‚’å–å¾—
        transcript_data = self.get_transcript(video_id)
        if not transcript_data:
            return []
        
        segments = []
        current_segment_start = 0.0
        current_texts = []
        prev_text = ""
        
        for i, entry in enumerate(transcript_data):
            text = self.clean_text(entry['text'])
            start_time = entry['start']
            
            # ç„¡è¦–ã™ã¹ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
            if self.should_ignore_segment(text):
                continue
            
            # è©±é¡Œè»¢æ›ã‚’åˆ¤å®š
            is_transition, confidence = self.is_topic_transition(text, prev_text)
            
            # è©±é¡Œè»¢æ›ãŒæ¤œå‡ºã•ã‚ŒãŸã€ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãŒè“„ç©ã•ã‚ŒãŸå ´åˆ
            if is_transition and current_texts:
                # ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç¢ºå®š
                combined_text = " ".join(current_texts)
                keywords = self.extract_keywords(combined_text)
                topic = self.classify_topic(combined_text, keywords)
                
                segment = TopicSegment(
                    start_time=current_segment_start,
                    end_time=start_time,
                    topic=topic,
                    keywords=keywords,
                    confidence=confidence
                )
                
                segments.append(segment)
                
                # æ–°ã—ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é–‹å§‹
                current_segment_start = start_time
                current_texts = [text]
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’è“„ç©
                current_texts.append(text)
                if not current_texts or len(current_texts) == 1:
                    current_segment_start = start_time
            
            prev_text = text
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†
        if current_texts:
            combined_text = " ".join(current_texts)
            keywords = self.extract_keywords(combined_text)
            topic = self.classify_topic(combined_text, keywords)
            
            # å‹•ç”»ã®æœ€å¾Œã®æ™‚é–“ã‚’æ¨å®š
            last_end_time = transcript_data[-1]['start'] + transcript_data[-1].get('duration', 5.0)
            
            segment = TopicSegment(
                start_time=current_segment_start,
                end_time=last_end_time,
                topic=topic,
                keywords=keywords,
                confidence=0.7
            )
            segments.append(segment)
        
        # çŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµ±åˆ
        segments = self.merge_short_segments(segments, min_segment_duration)
        
        print(f"{len(segments)}å€‹ã®è©±é¡Œã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        return segments
    
    def save_topics_to_csv(self, segments: List[TopicSegment], video_id: str, video_title: str = ""):
        """è©±é¡Œã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’CSVã«ä¿å­˜"""
        import csv
        
        filename = f"topics_{video_id}.csv"
        
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                "No", "é–‹å§‹æ™‚é–“", "çµ‚äº†æ™‚é–“", "é•·ã•(åˆ†)", "è©±é¡Œ", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", 
                "ä¿¡é ¼åº¦", "YouTubeãƒªãƒ³ã‚¯"
            ])
            
            for i, segment in enumerate(segments, 1):
                duration_minutes = segment.duration / 60
                youtube_url = f"https://www.youtube.com/watch?v={video_id}{segment.youtube_link}"
                
                writer.writerow([
                    i,
                    segment.start_timestamp,
                    segment.end_timestamp,
                    f"{duration_minutes:.1f}",
                    segment.topic,
                    ", ".join(segment.keywords),
                    f"{segment.confidence:.2f}",
                    youtube_url
                ])
        
        print(f"è©±é¡Œãƒªã‚¹ãƒˆã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        
        # çµ±è¨ˆè¡¨ç¤º
        topic_counts = {}
        total_duration = 0
        
        for segment in segments:
            topic_counts[segment.topic] = topic_counts.get(segment.topic, 0) + 1
            total_duration += segment.duration
        
        print(f"\nè©±é¡Œåˆ¥çµ±è¨ˆ:")
        for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   {topic}: {count}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
        
        print(f"ç·æ™‚é–“: {total_duration/60:.1f}åˆ†")

def main():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    analyzer = TranscriptTopicAnalyzer()
    
    # ãƒ†ã‚¹ãƒˆç”¨å‹•ç”»ID
    video_id = input("å‹•ç”»IDã¾ãŸã¯YouTube URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    
    # URLã‹ã‚‰å‹•ç”»IDã‚’æŠ½å‡º
    if "youtube.com" in video_id or "youtu.be" in video_id:
        import re
        match = re.search(r'(?:youtube\.com/watch\?v=|youtu\.be/)([a-zA-Z0-9_-]{11})', video_id)
        if match:
            video_id = match.group(1)
        else:
            print("ç„¡åŠ¹ãªYouTube URLã§ã™")
            return
    
    # è©±é¡Œåˆ†æã‚’å®Ÿè¡Œ
    segments = analyzer.analyze_topics(video_id)
    
    if segments:
        # çµæœè¡¨ç¤º
        print(f"\n=== æ¤œå‡ºã•ã‚ŒãŸè©±é¡Œã‚»ã‚°ãƒ¡ãƒ³ãƒˆ ===")
        for i, segment in enumerate(segments, 1):
            print(f"{i:2d}. {segment.start_timestamp}-{segment.end_timestamp} "
                  f"({segment.duration/60:.1f}åˆ†) {segment.topic}")
            if segment.keywords:
                print(f"     ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(segment.keywords)}")
            print(f"     ä¿¡é ¼åº¦: {segment.confidence:.2f}")
        
        # CSVã«ä¿å­˜
        analyzer.save_topics_to_csv(segments, video_id)
    else:
        print("è©±é¡Œã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main()