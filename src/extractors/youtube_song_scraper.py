import json
import os
import re
import csv
import sys
from datetime import datetime, timezone, timedelta
from dataclasses import asdict
from typing import List, Optional

from googleapiclient import discovery
from dotenv import load_dotenv

# Windowsç’°å¢ƒã§ã®cp932ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã®è¨­å®š
if sys.platform == 'win32':
    # æ¨™æº–å‡ºåŠ›ã‚’UTF-8ã«è¨­å®šï¼ˆPython 3.7+ï¼‰
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    if hasattr(sys.stderr, 'reconfigure'):
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')

def safe_print(text):
    """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹å®‰å…¨ãªprinté–¢æ•°"""
    try:
        print(text)
    except (UnicodeEncodeError, UnicodeDecodeError):
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§ããªã„æ–‡å­—ã‚’ç½®ãæ›ãˆã‚‹
        safe_text = str(text).encode('ascii', 'replace').decode('ascii')
        print(safe_text)

# MeCabã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import MeCab
    mecab_reading = MeCab.Tagger('-Oyomi')
    safe_print("MeCab loaded successfully")
except (ImportError, RuntimeError) as e:
    safe_print(f"MeCab not available: {type(e).__name__}. Using simple hiragana conversion.")
    mecab_reading = None

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from utils.infoclass import VideoInfo, CommentInfo, TimeStamp
from utils.utils import aligned_json_dump
from utils.genre_classifier import GenreClassifier
from utils.music_classifier import MusicClassifier

load_dotenv()
API_KEY = os.getenv('API_KEY')
if not API_KEY:
    raise RuntimeError("`.env` ã« API_KEY ãŒã‚ã‚Šã¾ã›ã‚“ã€‚YouTube Data API v3 ã®APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

youtube = discovery.build('youtube', 'v3', developerKey=API_KEY)

# å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿
try:
    user_data = json.load(open('user_ids.json', encoding='utf-8'))
    # æ–°å½¢å¼ï¼ˆè¾æ›¸å‹ï¼‰ã‹æ—§å½¢å¼ï¼ˆé…åˆ—å‹ï¼‰ã‹åˆ¤å®š
    if isinstance(user_data, dict):
        users = [ch['channel_id'] for ch in user_data.get('channels', []) if ch.get('enabled', True)]
    else:
        users = user_data  # æ—§å½¢å¼ï¼ˆé…åˆ—ï¼‰
except FileNotFoundError:
    safe_print("user_ids.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
    users = ["UCxxxxxxxxxxxxxxxxxxxxxx"]
    with open('user_ids.json', 'w', encoding='utf-8') as f:
        json.dump(users, f, ensure_ascii=False, indent=2)

class EnhancedAnalyzer:
    def __init__(self):
        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ†é¡å™¨ã‚’åˆæœŸåŒ–ï¼ˆJSONçµ±åˆç‰ˆï¼‰
        self.genre_classifier = GenreClassifier()

    def to_hiragana(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã²ã‚‰ãŒãªã«å¤‰æ›"""
        if mecab_reading:
            try:
                reading = mecab_reading.parse(text).strip()
                hiragana = ''
                for char in reading:
                    if 'ã‚¡' <= char <= 'ãƒ¶':
                        hiragana += chr(ord(char) - ord('ã‚¡') + ord('ã'))
                    elif char == 'ãƒµ':
                        hiragana += 'ã‹'
                    elif char == 'ãƒ¶':
                        hiragana += 'ã‘'
                    else:
                        hiragana += char.lower()
                return hiragana
            except:
                pass
        
        # MeCabãŒä½¿ãˆãªã„å ´åˆã®ç°¡æ˜“å¤‰æ›
        return self._simple_katakana_to_hiragana(text.lower())
    
    def _simple_katakana_to_hiragana(self, text: str) -> str:
        """ç°¡æ˜“ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›ï¼ˆè‹±æ•°å­—ãƒ»è¨˜å·ã‚‚å‡¦ç†ï¼‰"""
        result = ''
        for char in text:
            if 'ã‚¡' <= char <= 'ãƒ¶':
                result += chr(ord(char) - ord('ã‚¡') + ord('ã'))
            elif char == 'ãƒµ':
                result += 'ã‹'
            elif char == 'ãƒ¶':
                result += 'ã‘'
            elif 'A' <= char <= 'Z':
                result += char.lower()
            elif char in 'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™':
                # å…¨è§’æ•°å­—ã‚’åŠè§’ã«
                result += str(ord(char) - ord('ï¼'))
            elif char in 'ï¼ˆï¼‰ï¼»ï¼½ï½›ï½':
                # å…¨è§’æ‹¬å¼§ã‚’é™¤å»
                continue
            else:
                result += char
        return result

    def detect_genre(self, title: str, artist: str) -> str:
        """ã‚¸ãƒ£ãƒ³ãƒ«ã‚’è‡ªå‹•åˆ¤å®šï¼ˆJSONçµ±åˆç‰ˆï¼‰"""
        return self.genre_classifier.classify(artist, title)

    def calculate_confidence_score(self, video_info: VideoInfo, extracted_timestamps: list = None) -> float:
        """
        æ­Œå‹•ç”»ã®ç¢ºåº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆæ”¹å–„ç‰ˆï¼‰

        Args:
            video_info: å‹•ç”»æƒ…å ±
            extracted_timestamps: æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ãƒªã‚¹ãƒˆï¼ˆçœç•¥å¯ï¼‰

        Returns:
            0.0-1.0ã®ç¢ºåº¦ã‚¹ã‚³ã‚¢
        """
        title = video_info.title
        description = video_info.description

        # æ—¢å­˜ã®is_singing_streamé–¢æ•°ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯
        combined_text = f"{title} {description}".lower()
        singing_keywords = [
            "æ­Œ", "ã†ãŸ", "æ­Œæ ", "ã†ãŸã‚ã", "æ­Œé…ä¿¡", "singing", "sing",
            "ã‚«ãƒ©ã‚ªã‚±", "ã‹ã‚‰ãŠã‘", "karaoke",
            "éŸ³æ¥½", "music", "æ¥½æ›²", "ã‚½ãƒ³ã‚°", "song",
            "ãƒ¡ãƒ‰ãƒ¬ãƒ¼", "medley", "å¼¾ãèªã‚Š",
            "ãƒ©ã‚¤ãƒ–", "live", "æ¼”å¥", "performance",
            "ã‚¢ã‚«ãƒšãƒ©", "acappella", "ã‚³ãƒ¼ãƒ©ã‚¹", "chorus",
            "æ­Œã£ã¦ã¿ãŸ", "ã†ãŸã£ã¦ã¿ãŸ", "æ­Œãƒªãƒ¬ãƒ¼", "æ­Œå›",
            "ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ­Œ", "æ­Œç·´ç¿’", "æ–°æ›²", "cover",
            "ãƒœã‚«ãƒ­", "vocaloid", "ã‚¢ãƒ‹ã‚½ãƒ³", "anime song", "anisong",
            "ã‚»ãƒˆãƒª", "setlist", "ãƒªãƒ", "ãƒªãƒãƒ¼ã‚µãƒ«", "rehearsal"
        ]
        exclude_keywords = [
            "ã‚²ãƒ¼ãƒ ", "game", "gaming", "ãƒ—ãƒ¬ã‚¤", "play",
            "é›‘è«‡", "zatsudan", "talk", "ãŠã—ã‚ƒã¹ã‚Š", "chat",
            "æ–™ç†", "cooking", "ã‚¯ãƒƒã‚­ãƒ³ã‚°", "é£Ÿã¹ã‚‹", "eating",
            "ãŠçµµæã", "çµµ", "drawing", "art", "ã‚¤ãƒ©ã‚¹ãƒˆ",
            "å·¥ä½œ", "craft", "ä½œæ¥­", "work", "study", "å‹‰å¼·"
        ]

        singing_score = 0
        for keyword in singing_keywords:
            if keyword in combined_text:
                singing_score += 1

        exclude_score = 0
        for keyword in exclude_keywords:
            if keyword in combined_text:
                exclude_score += 1

        # ã‚¿ã‚¤ãƒˆãƒ«ã®é‡è¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé‡ã¿å¢—åŠ ï¼‰
        if re.search(r'[æ­Œã†ãŸã‚¦ã‚¿]', title):
            singing_score += 5  # 3â†’5ã«å¢—åŠ ï¼ˆæœ€ã‚‚ä¿¡é ¼ã§ãã‚‹ã‚·ã‚°ãƒŠãƒ«ï¼‰
        if re.search(r'[â™ªâ™«â™¬ğŸµğŸ¶ğŸ¤ğŸ¼]', combined_text):
            singing_score += 2

        timestamp_count = len(re.findall(r'\d{1,2}:\d{2}', description))
        if timestamp_count >= 3:
            singing_score += 2

        # ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã«ã‚ˆã‚‹è¿½åŠ ã‚¹ã‚³ã‚¢
        if hasattr(video_info, 'comments') and video_info.comments:
            comment_timestamp_count = 0
            song_format_count = 0

            for comment in video_info.comments:
                comment_text = comment.text_display if hasattr(comment, 'text_display') else str(comment)
                comment_timestamps = len(re.findall(r'\d{1,2}:\d{2}', comment_text))
                if comment_timestamps >= 3:
                    comment_timestamp_count += 1

                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— + ã€Œæ›²å / ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã€å½¢å¼ã‚’æ¤œå‡º
                # HTMLã‚¿ã‚°ã‚‚è€ƒæ…®ï¼ˆYouTubeã‚³ãƒ¡ãƒ³ãƒˆã¯<a>ã‚¿ã‚°ã‚’å«ã‚€ï¼‰
                if re.search(r'\d{1,2}:\d{2}(?::\d{2})?[^/\n]*/.+', comment_text):
                    song_format_count += 1

            # ã‚³ãƒ¡ãƒ³ãƒˆã«å¤šæ•°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒã‚ã‚‹å ´åˆã€æ­Œé…ä¿¡ã®å¯èƒ½æ€§ãŒé«˜ã„
            if comment_timestamp_count >= 2:
                singing_score += 4
            elif comment_timestamp_count >= 1:
                singing_score += 2

            # ã€Œæ›²å / ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã€å½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã€ã‚¹ã‚³ã‚¢è¿½åŠ 
            if song_format_count >= 3:
                singing_score += 6
            elif song_format_count >= 2:
                singing_score += 4
            elif song_format_count >= 1:
                singing_score += 2

        # â˜…æ–°æ©Ÿèƒ½: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®è³ªã‚’è©•ä¾¡ï¼ˆæœ€ã‚‚ä¿¡é ¼ã§ãã‚‹æŒ‡æ¨™ï¼‰
        timestamp_quality_score = 0
        if extracted_timestamps:
            # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåãŒã‚ã‚‹å‰²åˆ
            artist_count = sum(1 for ts in extracted_timestamps if '/' in ts.text)
            artist_ratio = artist_count / max(1, len(extracted_timestamps))

            if artist_ratio > 0.8:
                timestamp_quality_score += 10  # 80%ä»¥ä¸Šã«ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå = ç¢ºå®Ÿã«æ­Œæ 
            elif artist_ratio > 0.5:
                timestamp_quality_score += 6
            elif artist_ratio > 0.2:
                timestamp_quality_score += 3

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æ•°ï¼ˆå¤šã„ã»ã©ä¿¡é ¼ã§ãã‚‹ï¼‰
            ts_count = len(extracted_timestamps)
            if ts_count >= 20:
                timestamp_quality_score += 4
            elif ts_count >= 10:
                timestamp_quality_score += 3
            elif ts_count >= 5:
                timestamp_quality_score += 2
            elif ts_count >= 3:
                timestamp_quality_score += 1

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        raw_score = max(0, singing_score + timestamp_quality_score - exclude_score)

        # å‹•çš„ãªæ­£è¦åŒ–ï¼ˆæœ€å¤§ã‚¹ã‚³ã‚¢ã‚’æ¨å®šï¼‰
        # åŸºæœ¬ã‚¹ã‚³ã‚¢æœ€å¤§: 20ç‚¹ + ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è³ª: 14ç‚¹ + ã‚³ãƒ¡ãƒ³ãƒˆ: 10ç‚¹ = 44ç‚¹
        max_possible_score = 44
        normalized_score = min(1.0, raw_score / max_possible_score)

        return normalized_score

    def clean_title(self, text: str) -> str:
        """å…ˆé ­ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ã‚’é™¤å»"""
        # å…¨è§’æ•°å­—ã‚’åŠè§’ã«çµ±ä¸€
        text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))

        # ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¤‡æ•°å›é©ç”¨ã—ã¦å†å¸°çš„ã«é™¤å»ï¼‰
        # "01. æ›²å" "1) æ›²å" "ã€1ã€‘æ›²å" "(1) æ›²å" ãªã©
        # è¤‡æ•°ã®ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãŒé€£ç¶šã—ã¦ã„ã‚‹å ´åˆã‚‚ã‚ã‚‹ï¼ˆä¾‹: "01. 1) æ›²å"ï¼‰
        max_iterations = 3  # æœ€å¤§3å›ç¹°ã‚Šè¿”ã™

        for _ in range(max_iterations):
            original = text
            patterns = [
                r"^\s*\d{1,3}[\.\ã€‚\)ï¼‰\]ã€‘\-ãƒ¼ãƒ»]\s*",  # "01." "01ã€‚" "1)" "1ã€‘" "1-" "1ãƒ»" ãªã©ï¼ˆå…¨è§’ãƒ”ãƒªã‚ªãƒ‰ã‚‚å«ã‚€ï¼‰
                r"^\s*[\(\(ã€\[]\s*\d{1,3}\s*[\)\)ã€‘\]]\s*",  # "(1)" "ã€1ã€‘" "[1]" ãªã©
                r"^\s*\d{1,3}\s+",  # "01 " (æ•°å­—+ã‚¹ãƒšãƒ¼ã‚¹)
                r"^\s*[ç¬¬]\d{1,3}[æ›²è©±å›ç« ]\s*",  # "ç¬¬1æ›²" "ç¬¬1è©±" ãªã©
            ]

            for pattern in patterns:
                text = re.sub(pattern, "", text)

            # å¤‰åŒ–ãŒãªããªã£ãŸã‚‰çµ‚äº†
            if text == original:
                break

        text = re.sub(r"<br\s*/?>", " ", text, flags=re.IGNORECASE)

        # å…ˆé ­ã®è£…é£¾è¨˜å·ã‚’é™¤å»ï¼ˆ&, ï¼†, â€», â˜…, â˜†, â– , â–¡, â—†, â—‡, â—, â—‹, â–², â–³, â–¼, â–½ãªã©ï¼‰
        text = re.sub(r"^\s*[&ï¼†â€»â˜…â˜†â– â–¡â—†â—‡â—â—‹â–²â–³â–¼â–½â¤â¡â†’â‡’â–ºâ–¶â–ºãƒ»]+\s*", "", text)

        return text.strip()

    def is_valid_song_entry(self, title: str, artist: str) -> bool:
        """æœ‰åŠ¹ãªæ›²ã‚¨ãƒ³ãƒˆãƒªã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # æ›²åãŒç©ºã®å ´åˆã¯ç„¡åŠ¹
        if not title or not title.strip():
            return False

        # æ•°å­—ã¨è¨˜å·ã®ã¿ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ç„¡åŠ¹
        if re.match(r'^[\d\s\.\-\(\)\[\]ã€€]+$', title):
            return False

        # ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ï¼ˆ"01." "1)" ãªã©ï¼‰ã®å ´åˆã¯ç„¡åŠ¹
        if re.match(r'^\d+[\.\)\-\s]*$', title):
            return False

        # ç„¡åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ˜ã‚‰ã‹ã«ã‚´ãƒŸï¼‰
        invalid_patterns = [
            r'^ã‚»ãƒˆãƒª$',
            r'^ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—$',
            r'^ãƒªã‚¹ãƒˆ$',
            r'^æ›²ç›®$',
            r'^\d+æ›²ç›®$',
            r'^BGM$',
            r'å¾…æ©Ÿ',
            r'é…ä¿¡é–‹å§‹',
            r'ä¼‘æ†©',
            r'ã‚²ãƒ¼ãƒ ',
            r'é›‘è«‡',
            r'å®Ÿæ³',
            r'ãƒ†ã‚¹ãƒˆ',
            r'ãŠçŸ¥ã‚‰ã›',
            r'å‘ŠçŸ¥',
            r'^ğŸ¦‰',  # çµµæ–‡å­—ã§å§‹ã¾ã‚‹
            r'è¦‹ãˆã¦å®Ÿã¯',  # ã€Œå˜ç´”ãªã‚ˆã†ã«è¦‹ãˆã¦å®Ÿã¯...ã€ã¿ãŸã„ãªã®
            # åˆé…ä¿¡ãªã©ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆæ­Œã§ã¯ãªã„ï¼‰
            r'åˆé…ä¿¡',
            r'åˆ.*é…ä¿¡',  # ã€Œåˆæ­Œé…ä¿¡ã€ãªã©ã‚‚é™¤å¤–
            r'ç¬¬ä¸€å£°',
            r'è‡ªå·±ç´¹ä»‹',
            r'å…¬é–‹',
            r'ã«ã¤ã„ã¦',
            r'ç›®æ¨™',
            r'ä»Šå¾Œ',
            r'ä½œå“',
            r'ç”»ä¼¯',
            r'èªã‚‹',
            r'å¾—æ„',
        ]

        title_lower = title.lower()
        for pattern in invalid_patterns:
            if re.search(pattern, title, re.IGNORECASE):
                return False

        # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåãŒã‚ã‚‹å ´åˆã¯OK
        if artist and artist.strip():
            return True

        # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåãŒãªã„å ´åˆã¯ã€æ›²åã‚‰ã—ã•ã§åˆ¤å®š
        # 1. æ›²åãŒçŸ­ã™ãã‚‹ï¼ˆ2æ–‡å­—ä»¥ä¸‹ï¼‰å ´åˆã¯ç„¡åŠ¹
        if len(title.strip()) <= 2:
            return False

        # 2. æ—¥æœ¬èªã®æ›²åã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ãŒå«ã¾ã‚Œã‚‹ï¼‰
        if re.search(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]', title):
            return True

        # 3. è‹±èªã®æ›²åã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè‹±å­—ãŒä¸»ä½“ï¼‰
        if re.match(r'^[a-zA-Z\s\-\'.!?]+$', title) and len(title.strip()) >= 3:
            return True

        # ãã‚Œä»¥å¤–ã®ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåãªã—ã‚¨ãƒ³ãƒˆãƒªã¯ç„¡åŠ¹
        return False

    def parse_song_title_artist(self, title: str) -> tuple[str, str]:
        """æ›²åã¨ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã‚’åˆ†é›¢"""
        title = self.clean_title(title)

        # ã€Œæ›² / æ­Œæ‰‹ã€å½¢å¼ã§åˆ†å‰²
        parts = re.split(r"\s*/\s*", title, maxsplit=1)
        if len(parts) == 2:
            # åˆ†å‰²å¾Œã‚‚å„éƒ¨åˆ†ã«å¯¾ã—ã¦clean_titleã‚’é©ç”¨ï¼ˆãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãŒæ›²åå´ã«æ®‹ã£ã¦ã„ã‚‹å ´åˆï¼‰
            song_title = self.clean_title(parts[0].strip())
            artist = parts[1].strip()
            return song_title, artist
        else:
            return title.strip(), ""

def is_singing_stream(title: str, description: str, comments: Optional[List[str]] = None) -> bool:
    """æ­Œå‹•ç”»åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆåˆ†æå¼·åŒ–ç‰ˆï¼‰"""
    combined_text = f"{title} {description}".lower()
    singing_keywords = [
        "æ­Œ", "ã†ãŸ", "æ­Œæ ", "ã†ãŸã‚ã", "æ­Œé…ä¿¡", "singing", "sing",
        "ã‚«ãƒ©ã‚ªã‚±", "ã‹ã‚‰ãŠã‘", "karaoke",
        "éŸ³æ¥½", "music", "æ¥½æ›²", "ã‚½ãƒ³ã‚°", "song",
        "ãƒ¡ãƒ‰ãƒ¬ãƒ¼", "medley", "å¼¾ãèªã‚Š",
        "ãƒ©ã‚¤ãƒ–", "live", "æ¼”å¥", "performance",
        "ã‚¢ã‚«ãƒšãƒ©", "acappella", "ã‚³ãƒ¼ãƒ©ã‚¹", "chorus",
        "æ­Œã£ã¦ã¿ãŸ", "ã†ãŸã£ã¦ã¿ãŸ", "æ­Œãƒªãƒ¬ãƒ¼", "æ­Œå›",
        "ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ­Œ", "æ­Œç·´ç¿’", "æ–°æ›²", "cover",
        "ãƒœã‚«ãƒ­", "vocaloid", "ã‚¢ãƒ‹ã‚½ãƒ³", "anime song", "anisong",
        "ã‚»ãƒˆãƒª", "setlist", "ãƒªãƒ", "ãƒªãƒãƒ¼ã‚µãƒ«", "rehearsal"
    ]
    exclude_keywords = [
        "ã‚²ãƒ¼ãƒ ", "game", "gaming", "ãƒ—ãƒ¬ã‚¤", "play",
        "é›‘è«‡", "zatsudan", "talk", "ãŠã—ã‚ƒã¹ã‚Š", "chat",
        "æ–™ç†", "cooking", "ã‚¯ãƒƒã‚­ãƒ³ã‚°", "é£Ÿã¹ã‚‹", "eating",
        "ãŠçµµæã", "çµµ", "drawing", "art", "ã‚¤ãƒ©ã‚¹ãƒˆ",
        "å·¥ä½œ", "craft", "ä½œæ¥­", "work", "study", "å‹‰å¼·"
    ]
    singing_score = 0
    for keyword in singing_keywords:
        if keyword in combined_text:
            singing_score += 1
    exclude_score = 0
    for keyword in exclude_keywords:
        if keyword in combined_text:
            exclude_score += 1
    if re.search(r'[æ­Œã†ãŸã‚¦ã‚¿]', title):
        singing_score += 3
    if re.search(r'[â™ªâ™«â™¬ğŸµğŸ¶ğŸ¤ğŸ¼]', combined_text):
        singing_score += 2
    timestamp_count = len(re.findall(r'\d{1,2}:\d{2}', description))
    if timestamp_count >= 3:
        singing_score += 2

    # ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã«ã‚ˆã‚‹è¿½åŠ ã‚¹ã‚³ã‚¢
    if comments:
        comment_timestamp_count = 0
        song_format_count = 0  # ã€Œæ›²å / ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã€å½¢å¼ã®ã‚«ã‚¦ãƒ³ãƒˆ

        for comment in comments:
            comment_timestamps = len(re.findall(r'\d{1,2}:\d{2}', comment))
            if comment_timestamps >= 3:  # 1ã‚³ãƒ¡ãƒ³ãƒˆã«3ã¤ä»¥ä¸Šã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                comment_timestamp_count += 1

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— + ã€Œæ›²å / ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã€å½¢å¼ã‚’æ¤œå‡º
            # ä¾‹: "43:00 è¶ã€…çµã³ / Aimer" ã‚„ "1:23:45 æ›²å/æ­Œæ‰‹"
            # HTMLã‚¿ã‚°ã‚‚è€ƒæ…®ï¼ˆYouTubeã‚³ãƒ¡ãƒ³ãƒˆã¯<a>ã‚¿ã‚°ã‚’å«ã‚€ï¼‰
            if re.search(r'\d{1,2}:\d{2}(?::\d{2})?[^/\n]*/.+', comment):
                song_format_count += 1

        # ã‚³ãƒ¡ãƒ³ãƒˆã«å¤šæ•°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒã‚ã‚‹å ´åˆã€æ­Œé…ä¿¡ã®å¯èƒ½æ€§ãŒé«˜ã„
        if comment_timestamp_count >= 2:
            singing_score += 4
        elif comment_timestamp_count >= 1:
            singing_score += 2

        # ã€Œæ›²å / ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã€å½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã€æ­Œé…ä¿¡ã®å¯èƒ½æ€§ãŒéå¸¸ã«é«˜ã„
        if song_format_count >= 3:
            singing_score += 6  # å¼·ã„ä¿¡å·
        elif song_format_count >= 2:
            singing_score += 4
        elif song_format_count >= 1:
            singing_score += 2

    if singing_score >= 2 and exclude_score <= singing_score:
        return True
    elif singing_score >= 4:
        return True
    else:
        return False

def merge_with_existing_csv(csv_file: str, new_rows: list) -> list:
    """
    æ—¢å­˜CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡é™¤å»ï¼‰

    Args:
        csv_file: æ—¢å­˜CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        new_rows: æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿è¡Œã®ãƒªã‚¹ãƒˆ

    Returns:
        ãƒãƒ¼ã‚¸å¾Œã®ãƒ‡ãƒ¼ã‚¿è¡Œãƒªã‚¹ãƒˆ
    """
    if not os.path.exists(csv_file):
        return new_rows

    try:
        existing_rows = []
        with open(csv_file, 'r', encoding='utf-8-sig', newline='') as f:
            reader = csv.reader(f)
            header = next(reader, None)  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
            for row in reader:
                existing_rows.append(row)

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚­ãƒ¼ (å‹•ç”»ID + ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—)
        existing_keys = {(row[7], row[5]) for row in existing_rows}  # (å‹•ç”»ID, ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—)
        new_unique_rows = []

        for row in new_rows:
            key = (row[7], row[5])
            if key not in existing_keys:
                new_unique_rows.append(row)
                existing_keys.add(key)

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        merged = existing_rows + new_unique_rows

        # é…ä¿¡æ—¥ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰
        merged.sort(key=lambda x: (x[6], x[5]))  # é…ä¿¡æ—¥ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ

        # é€£ç•ªã‚’æŒ¯ã‚Šç›´ã™
        for i, row in enumerate(merged, 1):
            row[0] = i

        safe_print(f"  æ—¢å­˜: {len(existing_rows)}ä»¶, æ–°è¦: {len(new_unique_rows)}ä»¶, åˆè¨ˆ: {len(merged)}ä»¶")
        return merged

    except Exception as e:
        safe_print(f"  [!] CSVãƒãƒ¼ã‚¸ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return new_rows

def get_uploads_playlist_id(channel_id: str) -> str | None:
    """æ—¢å­˜é–¢æ•°ã‚’ãã®ã¾ã¾ä½¿ç”¨"""
    if not channel_id or not channel_id.startswith("UC"):
        return None
    try:
        resp = youtube.channels().list(
            part="contentDetails",
            id=channel_id,
            fields="items/contentDetails/relatedPlaylists/uploads"
        ).execute()
        items = resp.get("items", [])
        if not items:
            return None
        return items[0]["contentDetails"]["relatedPlaylists"]["uploads"]
    except Exception as e:
        safe_print(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id} ã® uploads ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_video_info_in_playlist(playlist_id: str, published_after: str = None) -> list[VideoInfo]:
    """
    ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‹ã‚‰å‹•ç”»æƒ…å ±ã‚’å–å¾—ï¼ˆå·®åˆ†æ›´æ–°å¯¾å¿œï¼‰

    Args:
        playlist_id: ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆID
        published_after: ã“ã®æ—¥ä»˜ä»¥é™ã®å‹•ç”»ã®ã¿å–å¾—ï¼ˆISO 8601å½¢å¼ï¼‰
    """
    video_info_list: list[VideoInfo] = []
    try:
        request = youtube.playlistItems().list(
            part="snippet",
            playlistId=playlist_id,
            maxResults=50,
            fields="nextPageToken,items/snippet(publishedAt,title,description,resourceId/videoId)"
        )

        filter_date = None
        if published_after:
            filter_date = datetime.fromisoformat(published_after.replace("Z", "+00:00"))

        while request:
            response = request.execute()
            items = response.get("items", [])

            should_break = False
            for i in items:
                vi = VideoInfo.from_response_snippet(i["snippet"])
                vid = vi.id

                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¤ã„å‹•ç”»ãŒå‡ºã¦ããŸã‚‰çµ‚äº†ï¼‰
                if filter_date:
                    try:
                        video_date = datetime.fromisoformat(vi.published_at.replace("Z", "+00:00"))
                        if video_date < filter_date:
                            safe_print(f"  âœ“ {filter_date.strftime('%Y-%m-%d')} ã‚ˆã‚Šå‰ã®å‹•ç”»ã«åˆ°é”ã€å‡¦ç†çµ‚äº†")
                            should_break = True
                            break
                    except Exception as e:
                        safe_print(f"  ! æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")

                # --- å‹•ç”»è©³ç´°ã‚’è¿½åŠ ã§å–å¾— ---
                try:
                    details = youtube.videos().list(
                        part="liveStreamingDetails,snippet",
                        id=vid,
                        fields="items(snippet/publishedAt,liveStreamingDetails/actualStartTime)"
                    ).execute()

                    if details.get("items"):
                        item = details["items"][0]
                        vi.stream_start = item.get("liveStreamingDetails", {}).get("actualStartTime")
                        if not vi.stream_start:
                            vi.stream_start = item["snippet"]["publishedAt"]

                except Exception as e:
                    safe_print(f"å‹•ç”» {vid} ã®è©³ç´°å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")

                video_info_list.append(vi)

            if should_break:
                break

            request = youtube.playlistItems().list_next(request, response)
    except Exception as e:
        safe_print(f"ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ {playlist_id} ã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
    return video_info_list

def get_comments(video_id: str) -> list[CommentInfo]:
    """æ—¢å­˜é–¢æ•°ã‚’ãã®ã¾ã¾ä½¿ç”¨"""
    comment_list: list[CommentInfo] = []
    comment_field = "snippet(videoId,textDisplay,textOriginal)"
    top_comment_f = f"items/snippet/topLevelComment/{comment_field}"
    replies_f = f"items/replies/comments/{comment_field}"

    try:
        request = youtube.commentThreads().list(
            part="snippet,replies",
            maxResults=100,
            videoId=video_id,
            fields=f"nextPageToken,{top_comment_f},{replies_f}"
        )
        while request:
            response = request.execute()
            for item in response.get("items", []):
                comment_list.extend(CommentInfo.response_item_to_comments(item))
            request = youtube.commentThreads().list_next(request, response)
    except Exception as e:
        safe_print(f"å‹•ç”» {video_id} ã®ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")

    return comment_list

def scrape_channels(channel_ids: List[str], output_file: str = "output/csv/song_timestamps_complete.csv", filter_singing_only: bool = False, incremental: bool = True):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒ³ãƒãƒ«IDãƒªã‚¹ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã™ã‚‹

    Args:
        channel_ids: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã™ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«IDã®ãƒªã‚¹ãƒˆ
        output_file: å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯output/csv/ã«ä¿å­˜ï¼‰
        filter_singing_only: Trueã®å ´åˆã¯æ­Œæ ã®ã¿ã€Falseã®å ´åˆã¯ã™ã¹ã¦ã®å‹•ç”»ã‚’å¯¾è±¡
        incremental: Trueã®å ´åˆã¯å·®åˆ†æ›´æ–°ã€Falseã®å ´åˆã¯å…¨ä»¶å–å¾—
    """
    mode_text = "ã€æ­Œæ ãƒ¢ãƒ¼ãƒ‰ã€‘" if filter_singing_only else "ã€ç·åˆãƒ¢ãƒ¼ãƒ‰ã€‘"
    update_text = "ã€å·®åˆ†æ›´æ–°ã€‘" if incremental else "ã€å…¨ä»¶å–å¾—ã€‘"
    safe_print(f"YouTubeã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡ºãƒ„ãƒ¼ãƒ« {mode_text} {update_text}")
    safe_print("=" * 60)
    safe_print(f"å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«æ•°: {len(channel_ids)}")
    safe_print("")

    analyzer = EnhancedAnalyzer()

    # å‰å›å®Ÿè¡Œæ—¥æ™‚ã‚’èª­ã¿è¾¼ã‚€
    published_after = None
    if incremental:
        try:
            with open('last_scrape.json', 'r', encoding='utf-8') as f:
                last_scrape_data = json.load(f)
                last_run = last_scrape_data.get('last_run')
                if last_run:
                    published_after = last_run
                    safe_print(f"[å·®åˆ†æ›´æ–°] {last_run} ä»¥é™ã®å‹•ç”»ã‚’å–å¾—ã—ã¾ã™")
                else:
                    safe_print("[å·®åˆ†æ›´æ–°] åˆå›å®Ÿè¡Œã®ãŸã‚å…¨å‹•ç”»ã‚’å–å¾—ã—ã¾ã™")
        except FileNotFoundError:
            safe_print("[å·®åˆ†æ›´æ–°] last_scrape.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨å‹•ç”»ã‚’å–å¾—ã—ã¾ã™")

    # 1. å‹•ç”»æƒ…å ±å–å¾—
    uploads_ids: list[str] = []
    for uc in channel_ids:
        up = get_uploads_playlist_id(uc)
        if up:
            uploads_ids.append(up)
        else:
            safe_print(f"å–å¾—å¤±æ•—: {uc}")

    video_info_list: list[VideoInfo] = []
    for upid in uploads_ids:
        video_info_list += get_video_info_in_playlist(upid, published_after=published_after)

    # 2. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if filter_singing_only:
        # æ­Œæ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ­Œæ ã®ã¿ï¼‰
        filtered_video_list = []
        for vi in video_info_list:
            # æ­Œæ åˆ¤å®š or æ¦‚è¦æ¬„ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒ1ã¤ä»¥ä¸Šã‚ã‚‹å ´åˆã¯é€šã™
            has_timestamp_in_desc = len(re.findall(r'\d{1,2}:\d{2}', vi.description)) >= 1
            # åˆé…ä¿¡ãªã©ç‰¹åˆ¥ãªå‹•ç”»ã‚‚é€šã™ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒã‚ã‚‹å¯èƒ½æ€§ï¼‰
            is_debut_or_special = bool(re.search(r'åˆé…ä¿¡|debut|åˆ.*é…ä¿¡', vi.title, re.IGNORECASE))
            
            if is_singing_stream(vi.title, vi.description) or has_timestamp_in_desc or is_debut_or_special:
                filtered_video_list.append(vi)
        safe_print(f"å…¨å‹•ç”»æ•°: {len(video_info_list)}, æ­Œæ å‹•ç”»æ•°: {len(filtered_video_list)}")
        safe_print("\n=== æ­Œæ ã¨ã—ã¦æ¤œå‡ºã•ã‚ŒãŸå‹•ç”» ===")
    else:
        # ã™ã¹ã¦ã®å‹•ç”»ã‚’å¯¾è±¡
        filtered_video_list = []
        for vi in video_info_list:
            filtered_video_list.append(vi)
        safe_print(f"å…¨å‹•ç”»æ•°: {len(video_info_list)}, å‡¦ç†å¯¾è±¡å‹•ç”»æ•°: {len(filtered_video_list)}")
        safe_print("\n=== å‡¦ç†å¯¾è±¡ã®å‹•ç”» ===")
    for i, vi in enumerate(filtered_video_list[:10]):
        try:
            safe_print(f"{i+1}. {vi.title}")
        except UnicodeEncodeError:
            safe_title = vi.title.encode('ascii', 'ignore').decode('ascii')
            safe_print(f"{i+1}. {safe_title} [...]")
    if len(filtered_video_list) > 10:
        safe_print(f"... ä»– {len(filtered_video_list) - 10} ä»¶")

    # 3. ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— + å†ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    safe_print("\nã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ä¸­...")
    filter_singing_only = False  # ã™ã¹ã¦ã®å‹•ç”»ã‚’å¯¾è±¡ã¨ã™ã‚‹
    secondary_filtered_list = []
    for i, video_info in enumerate(filtered_video_list):
        try:
            safe_print(f"{i+1}/{len(filtered_video_list)}: {video_info.title}")
        except UnicodeEncodeError:
            safe_print(f"{i+1}/{len(filtered_video_list)}: [title with emoji]")
        video_info.comments = get_comments(video_info.id)

        if filter_singing_only:
            # æ­Œæ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼šã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã§å†åˆ¤å®š
            comment_texts = [c.text_display for c in video_info.comments] if video_info.comments else []
            if is_singing_stream(video_info.title, video_info.description, comment_texts):
                secondary_filtered_list.append(video_info)
            else:
                safe_print(f"  â†’ ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã«ã‚ˆã‚Šé™¤å¤–")
        else:
            # ã™ã¹ã¦ã®å‹•ç”»ã‚’é€šã™
            secondary_filtered_list.append(video_info)

    filtered_video_list = secondary_filtered_list
    if filter_singing_only:
        safe_print(f"\nã‚³ãƒ¡ãƒ³ãƒˆåˆ†æå¾Œã®æ­Œæ å‹•ç”»æ•°: {len(filtered_video_list)}")
    else:
        safe_print(f"\nå‡¦ç†å¯¾è±¡å‹•ç”»æ•°: {len(filtered_video_list)}")

    # 4. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º
    safe_print("\nã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡ºä¸­...")
    all_timestamps = []
    video_timestamps_map = {}  # å‹•ç”»IDã”ã¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä¿æŒ

    for v in filtered_video_list:
        ts_list = TimeStamp.from_videoinfo(v)
        all_timestamps.extend(ts_list)
        video_timestamps_map[v.id] = ts_list  # å‹•ç”»ã”ã¨ã«ä¿å­˜

    safe_print(f"æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•°: {len(all_timestamps)}")

    # 5. CSVå½¢å¼ã«å¤‰æ›ï¼ˆé‡è¤‡é™¤å»å¼·åŒ–ç‰ˆï¼‰
    safe_print("\nCSVå½¢å¼ã«å¤‰æ›ä¸­...")
    rows = []
    seen = {}
    duplicate_groups = {}
    idx = 1

    for entry in all_timestamps:
        video_id = entry.video_id
        raw_title = entry.text
        timestamp = entry.timestamp
        published_at = getattr(entry, 'stream_start', None) or entry.published_at

        confidence = 0.0
        for vi in filtered_video_list:
            if vi.id == video_id:
                # æ”¹å–„ç‰ˆï¼šå‹•ç”»ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ¸¡ã™
                ts_for_video = video_timestamps_map.get(video_id, [])
                confidence = analyzer.calculate_confidence_score(vi, ts_for_video)
                break

        song_title, artist = analyzer.parse_song_title_artist(raw_title)

        if not analyzer.is_valid_song_entry(song_title, artist):
            continue

        time_parts = timestamp.split(':')
        total_seconds = 0
        try:
            if len(time_parts) == 2:
                total_seconds = int(time_parts[0]) * 60 + int(time_parts[1])
            elif len(time_parts) == 3:
                total_seconds = int(time_parts[0]) * 3600 + int(time_parts[1]) * 60 + int(time_parts[2])
        except:
            total_seconds = 0

        normalized_key = (
            song_title.lower().strip(),
            artist.lower().strip(),
            video_id,
            total_seconds // 5
        )

        if normalized_key not in duplicate_groups:
            duplicate_groups[normalized_key] = []

        duplicate_groups[normalized_key].append({
            'raw_title': raw_title,
            'song_title': song_title,
            'artist': artist,
            'timestamp': timestamp,
            'total_seconds': total_seconds,
            'video_id': video_id,
            'published_at': published_at,
            'confidence': confidence,
            'has_numbering': bool(re.match(r"^\s*\d+", raw_title))
        })

    # éŸ³æ¥½åˆ†é¡å™¨ã‚’åˆæœŸåŒ–
    music_classifier = MusicClassifier(request_delay=3.0)

    safe_print("\n[*] ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’åˆ†é¡ä¸­...")
    for normalized_key, duplicates in duplicate_groups.items():
        best = max(duplicates, key=lambda x: (
            not x['has_numbering'],
            len(x['song_title']),
            len(x['artist'])
        ))

        # éŸ³æ¥½ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ±ã‚’è£œå®Œ
        classification = music_classifier.classify_timestamp(
            best['song_title'],
            best['artist'],
            use_itunes=False  # iTunes APIç„¡åŠ¹åŒ–ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
        )

        genre = analyzer.detect_genre(classification['title'], classification['artist'])
        search_text = analyzer.to_hiragana(classification['title'])

        try:
            dt = datetime.fromisoformat((best['published_at'] or "").replace("Z", "+00:00"))
            date_str = dt.astimezone(timezone(timedelta(hours=9))).strftime("%Y/%m/%d")
        except Exception:
            date_str = ""

        row_data = [
            idx,
            classification['title'],
            classification['artist'],
            search_text,
            genre,
            best['timestamp'],
            date_str,
            best['video_id'],
            f"{best['confidence']:.2f}",
            best['total_seconds'],
            classification['is_music']  # éŸ³æ¥½ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ 
        ]
        rows.append(row_data)
        idx += 1

    rows.sort(key=lambda x: (x[6], x[9]))

    # æ­Œã¨ãã®ä»–ã«åˆ†é¡
    singing_rows = []
    other_rows = []

    for i, row in enumerate(rows, 1):
        row[0] = i
        is_music = row.pop()  # is_musicãƒ•ãƒ©ã‚°ã‚’å–ã‚Šå‡ºã™
        total_seconds = row.pop()  # total_secondsã‚’å‰Šé™¤

        if is_music:
            singing_rows.append(row)
        else:
            other_rows.append(row)

    # å†åº¦é€£ç•ªã‚’æŒ¯ã‚Šç›´ã™
    for i, row in enumerate(singing_rows, 1):
        row[0] = i
    for i, row in enumerate(other_rows, 1):
        row[0] = i

    # 6. æ—¢å­˜CSVã¨ãƒãƒ¼ã‚¸ï¼ˆå·®åˆ†æ›´æ–°ã®å ´åˆï¼‰
    output_dir = os.path.dirname(output_file)
    os.makedirs(output_dir, exist_ok=True)

    output_singing = os.path.join(output_dir, "song_timestamps_singing_only.csv")
    output_other = os.path.join(output_dir, "song_timestamps_other.csv")

    if incremental:
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒ¼ã‚¸
        singing_rows = merge_with_existing_csv(output_singing, singing_rows)
        other_rows = merge_with_existing_csv(output_other, other_rows)
        safe_print(f"\n[å·®åˆ†æ›´æ–°] æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸ã—ã¾ã—ãŸ")

    with open(output_singing, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["No","æ›²","æ­Œæ‰‹-ãƒ¦ãƒ‹ãƒƒãƒˆ","æ¤œç´¢ç”¨","ã‚¸ãƒ£ãƒ³ãƒ«","ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—","é…ä¿¡æ—¥","å‹•ç”»ID","ç¢ºåº¦ã‚¹ã‚³ã‚¢"])
        writer.writerows(singing_rows)

    with open(output_other, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["No","æ›²","æ­Œæ‰‹-ãƒ¦ãƒ‹ãƒƒãƒˆ","æ¤œç´¢ç”¨","ã‚¸ãƒ£ãƒ³ãƒ«","ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—","é…ä¿¡æ—¥","å‹•ç”»ID","ç¢ºåº¦ã‚¹ã‚³ã‚¢"])
        writer.writerows(other_rows)

    rows = singing_rows + other_rows  # çµ±è¨ˆè¡¨ç¤ºç”¨ã«çµåˆ

    safe_print(f"\nå®Œäº†ï¼CSVã‚’å‡ºåŠ›ã—ã¾ã—ãŸ:")
    safe_print(f"   - æ­Œæ : {output_singing} ({len(singing_rows)}ä»¶)")
    safe_print(f"   - ãã®ä»–: {output_other} ({len(other_rows)}ä»¶)")
    safe_print(f"\nçµ±è¨ˆ:")
    safe_print(f"   - å‡¦ç†ã—ãŸå‹•ç”»æ•°: {len(filtered_video_list)}")
    safe_print(f"   - æŠ½å‡ºã—ãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•°: {len(all_timestamps)}")
    safe_print(f"   - æœ€çµ‚å‡ºåŠ›è¡Œæ•°: {len(rows)}")

    if rows:
        # ç¢ºåº¦ã‚¹ã‚³ã‚¢çµ±è¨ˆ
        scores = [float(row[8]) for row in rows]
        high_conf = len([s for s in scores if s > 0.7])
        med_conf = len([s for s in scores if 0.4 <= s <= 0.7])
        low_conf = len([s for s in scores if s < 0.4])

        safe_print(f"\n   ç¢ºåº¦ã‚¹ã‚³ã‚¢åˆ†å¸ƒ:")
        safe_print(f"   - é«˜ç¢ºåº¦ (>0.7): {high_conf}ä»¶ ({high_conf/len(rows)*100:.1f}%)")
        safe_print(f"   - ä¸­ç¢ºåº¦ (0.4-0.7): {med_conf}ä»¶ ({med_conf/len(rows)*100:.1f}%)")
        safe_print(f"   - ä½ç¢ºåº¦ (<0.4): {low_conf}ä»¶ ({low_conf/len(rows)*100:.1f}%)")
        safe_print(f"   - å¹³å‡ç¢ºåº¦: {sum(scores)/len(scores):.2f}")

        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥çµ±è¨ˆ
        genre_stats = {}
        for row in rows:
            genre = row[4]  # ã‚¸ãƒ£ãƒ³ãƒ«åˆ—
            genre_stats[genre] = genre_stats.get(genre, 0) + 1

        safe_print(f"\n   ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥çµ±è¨ˆ:")
        for genre, count in sorted(genre_stats.items(), key=lambda x: x[1], reverse=True):
            safe_print(f"   - {genre}: {count}æ›² ({count/len(rows)*100:.1f}%)")

    vi_dict = [asdict(vi) for vi in filtered_video_list]
    aligned_json_dump(vi_dict, "output/json/comment_info.json")
    safe_print(f"\nãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—JSONã‚‚ä½œæˆ: output/json/comment_info.json")

    # å®Ÿè¡Œæ—¥æ™‚ã‚’ä¿å­˜ï¼ˆæ¬¡å›ã®å·®åˆ†æ›´æ–°ç”¨ï¼‰
    if incremental:
        now = datetime.now(timezone.utc).isoformat()
        with open('last_scrape.json', 'w', encoding='utf-8') as f:
            json.dump({
                'last_run': now,
                'note': 'ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€å¾Œã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ãŸæ—¥æ™‚ã‚’è¨˜éŒ²ã—ã¾ã™'
            }, f, ensure_ascii=False, indent=2)
        safe_print(f"\n[å·®åˆ†æ›´æ–°] æ¬¡å›å®Ÿè¡Œæ™‚ã¯ {now} ä»¥é™ã®å‹•ç”»ã‚’å–å¾—ã—ã¾ã™")


def main():
    safe_print("YouTubeæ­Œå‹•ç”»ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡ºãƒ„ãƒ¼ãƒ«ï¼ˆçµ±åˆç‰ˆï¼‰")
    safe_print("=" * 60)

    analyzer = EnhancedAnalyzer()

    # 1. å‹•ç”»æƒ…å ±å–å¾—ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    uploads_ids: list[str] = []
    for uc in users:
        up = get_uploads_playlist_id(uc)
        if up:
            uploads_ids.append(up)
        else:
            safe_print(f"å–å¾—å¤±æ•—: {uc}")

    video_info_list: list[VideoInfo] = []
    for upid in uploads_ids:
        video_info_list += get_video_info_in_playlist(upid)

    # 2. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã™ã¹ã¦ã®å‹•ç”»ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡ºï¼‰
    # æ­Œæ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ã—ã€ã™ã¹ã¦ã®å‹•ç”»ã‚’å¯¾è±¡ã¨ã™ã‚‹
    filtered_video_list = []
    for vi in video_info_list:
        # ã™ã¹ã¦ã®å‹•ç”»ã‚’é€šã™ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒã‚ã‚Œã°æŠ½å‡ºï¼‰
        filtered_video_list.append(vi)

    safe_print(f"å…¨å‹•ç”»æ•°: {len(video_info_list)}, å‡¦ç†å¯¾è±¡å‹•ç”»æ•°: {len(filtered_video_list)}")

    safe_print("\n=== å‡¦ç†å¯¾è±¡ã®å‹•ç”» ===")
    for i, vi in enumerate(filtered_video_list[:10]):
        try:
            safe_print(f"{i+1}. {vi.title}")
        except UnicodeEncodeError:
            # çµµæ–‡å­—ãªã©ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤º
            safe_title = vi.title.encode('ascii', 'ignore').decode('ascii')
            safe_print(f"{i+1}. {safe_title} [...]")
    if len(filtered_video_list) > 10:
        safe_print(f"... ä»– {len(filtered_video_list) - 10} ä»¶")

    # 3. ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— + å†ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    safe_print("\nã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ä¸­...")
    filter_singing_only = False  # ã™ã¹ã¦ã®å‹•ç”»ã‚’å¯¾è±¡ã¨ã™ã‚‹
    secondary_filtered_list = []
    for i, video_info in enumerate(filtered_video_list):
        try:
            safe_print(f"{i+1}/{len(filtered_video_list)}: {video_info.title}")
        except UnicodeEncodeError:
            safe_print(f"{i+1}/{len(filtered_video_list)}: [title with emoji]")
        video_info.comments = get_comments(video_info.id)

        if filter_singing_only:
            # æ­Œæ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼šã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã§å†åˆ¤å®š
            comment_texts = [c.text_display for c in video_info.comments] if video_info.comments else []
            if is_singing_stream(video_info.title, video_info.description, comment_texts):
                secondary_filtered_list.append(video_info)
            else:
                safe_print(f"  â†’ ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã«ã‚ˆã‚Šé™¤å¤–")
        else:
            # ã™ã¹ã¦ã®å‹•ç”»ã‚’é€šã™
            secondary_filtered_list.append(video_info)

    filtered_video_list = secondary_filtered_list
    if filter_singing_only:
        safe_print(f"\nã‚³ãƒ¡ãƒ³ãƒˆåˆ†æå¾Œã®æ­Œæ å‹•ç”»æ•°: {len(filtered_video_list)}")
    else:
        safe_print(f"\nå‡¦ç†å¯¾è±¡å‹•ç”»æ•°: {len(filtered_video_list)}")

    # 4. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º
    safe_print("\nã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡ºä¸­...")
    all_timestamps = []
    video_timestamps_map = {}  # å‹•ç”»IDã”ã¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä¿æŒ

    for v in filtered_video_list:
        ts_list = TimeStamp.from_videoinfo(v)
        all_timestamps.extend(ts_list)
        video_timestamps_map[v.id] = ts_list  # å‹•ç”»ã”ã¨ã«ä¿å­˜

    safe_print(f"æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•°: {len(all_timestamps)}")

    # 5. CSVå½¢å¼ã«å¤‰æ›ï¼ˆé‡è¤‡é™¤å»å¼·åŒ–ç‰ˆï¼‰
    safe_print("\nCSVå½¢å¼ã«å¤‰æ›ä¸­...")
    rows = []
    seen = {}
    duplicate_groups = {}  # é‡è¤‡ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    idx = 1

    # ç¬¬1ãƒ‘ã‚¹: ã™ã¹ã¦ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    for entry in all_timestamps:
        video_id = entry.video_id
        raw_title = entry.text
        timestamp = entry.timestamp
        published_at = getattr(entry, 'stream_start', None) or entry.published_at

        # ç¢ºåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆè©²å½“ã™ã‚‹å‹•ç”»ã‚’è¦‹ã¤ã‘ã¦è¨ˆç®—ï¼‰
        confidence = 0.0
        for vi in filtered_video_list:
            if vi.id == video_id:
                # æ”¹å–„ç‰ˆï¼šå‹•ç”»ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ¸¡ã™
                ts_for_video = video_timestamps_map.get(video_id, [])
                confidence = analyzer.calculate_confidence_score(vi, ts_for_video)
                break

        song_title, artist = analyzer.parse_song_title_artist(raw_title)

        # ç„¡åŠ¹ãªã‚¨ãƒ³ãƒˆãƒªã¯é™¤å¤–ï¼ˆæ­Œæ‰‹ãªã—ã€ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ã®ã¿ã€ãªã©ï¼‰
        if not analyzer.is_valid_song_entry(song_title, artist):
            continue

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç§’ã«å¤‰æ›ï¼ˆÂ±5ç§’ä»¥å†…ã¯åŒã˜ã¨ã¿ãªã™ï¼‰
        time_parts = timestamp.split(':')
        total_seconds = 0
        try:
            if len(time_parts) == 2:  # mm:ss
                total_seconds = int(time_parts[0]) * 60 + int(time_parts[1])
            elif len(time_parts) == 3:  # hh:mm:ss
                total_seconds = int(time_parts[0]) * 3600 + int(time_parts[1]) * 60 + int(time_parts[2])
        except:
            total_seconds = 0

        # æ­£è¦åŒ–ã‚­ãƒ¼ï¼ˆæ›²åã¨ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã®é¡ä¼¼æ€§ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®è¿‘ã•ã§åˆ¤å®šï¼‰
        normalized_key = (
            song_title.lower().strip(),
            artist.lower().strip(),
            video_id,
            total_seconds // 5  # 5ç§’å˜ä½ã§ä¸¸ã‚ã‚‹
        )

        if normalized_key not in duplicate_groups:
            duplicate_groups[normalized_key] = []

        duplicate_groups[normalized_key].append({
            'raw_title': raw_title,
            'song_title': song_title,
            'artist': artist,
            'timestamp': timestamp,
            'total_seconds': total_seconds,
            'video_id': video_id,
            'published_at': published_at,
            'confidence': confidence,
            'has_numbering': bool(re.match(r"^\s*\d+", raw_title))
        })

    # éŸ³æ¥½åˆ†é¡å™¨ã‚’åˆæœŸåŒ–
    music_classifier = MusicClassifier(request_delay=3.0)

    safe_print("\n[*] ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’åˆ†é¡ä¸­...")
    # ç¬¬2ãƒ‘ã‚¹: å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æœ€é©ãªã‚‚ã®ã‚’é¸æŠã—ã€åˆ†é¡
    for normalized_key, duplicates in duplicate_groups.items():
        # å„ªå…ˆé †ä½: ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãªã— > è©³ç´°ãªæ›²å > é•·ã„æ›²å
        best = max(duplicates, key=lambda x: (
            not x['has_numbering'],  # ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãŒãªã„æ–¹ãŒå„ªå…ˆ
            len(x['song_title']),     # æ›²åãŒé•·ã„æ–¹ãŒè©³ç´°
            len(x['artist'])          # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåãŒé•·ã„æ–¹ãŒè©³ç´°
        ))

        # éŸ³æ¥½ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæƒ…å ±ã‚’è£œå®Œ
        classification = music_classifier.classify_timestamp(
            best['song_title'],
            best['artist'],
            use_itunes=True
        )

        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®š
        genre = analyzer.detect_genre(classification['title'], classification['artist'])

        # ã²ã‚‰ãŒãªå¤‰æ›
        search_text = analyzer.to_hiragana(classification['title'])

        # æ—¥ä»˜ã‚’JSTã¸
        try:
            dt = datetime.fromisoformat((best['published_at'] or "").replace("Z", "+00:00"))
            date_str = dt.astimezone(timezone(timedelta(hours=9))).strftime("%Y/%m/%d")
        except Exception:
            date_str = ""

        rows.append([
            idx,
            classification['title'],
            classification['artist'],
            search_text,
            genre,
            best['timestamp'],
            date_str,
            best['video_id'],
            f"{best['confidence']:.2f}",
            best['total_seconds'],  # ã‚½ãƒ¼ãƒˆç”¨ã«è¿½åŠ ï¼ˆCSVå‡ºåŠ›æ™‚ã«ã¯é™¤å¤–ï¼‰
            classification['is_music']  # éŸ³æ¥½ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ 
        ])
        idx += 1

    # é…ä¿¡æ—¥ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰
    rows.sort(key=lambda x: (x[6], x[9]))  # é…ä¿¡æ—¥ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆç§’ï¼‰ã§ã‚½ãƒ¼ãƒˆ

    # æ­Œã¨ãã®ä»–ã«åˆ†é¡
    singing_rows = []
    other_rows = []

    for i, row in enumerate(rows, 1):
        row[0] = i
        is_music = row.pop()  # is_musicãƒ•ãƒ©ã‚°ã‚’å–ã‚Šå‡ºã™
        total_seconds = row.pop()  # total_secondsã‚’å‰Šé™¤

        if is_music:
            singing_rows.append(row)
        else:
            other_rows.append(row)

    # å†åº¦é€£ç•ªã‚’æŒ¯ã‚Šç›´ã™
    for i, row in enumerate(singing_rows, 1):
        row[0] = i
    for i, row in enumerate(other_rows, 1):
        row[0] = i

    # 6. CSVå‡ºåŠ›ï¼ˆ2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    output_dir = "output/csv"
    os.makedirs(output_dir, exist_ok=True)

    output_singing = os.path.join(output_dir, "song_timestamps_singing_only.csv")
    output_other = os.path.join(output_dir, "song_timestamps_other.csv")

    with open(output_singing, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["No","æ›²","æ­Œæ‰‹-ãƒ¦ãƒ‹ãƒƒãƒˆ","æ¤œç´¢ç”¨","ã‚¸ãƒ£ãƒ³ãƒ«","ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—","é…ä¿¡æ—¥","å‹•ç”»ID","ç¢ºåº¦ã‚¹ã‚³ã‚¢"])
        writer.writerows(singing_rows)

    with open(output_other, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["No","æ›²","æ­Œæ‰‹-ãƒ¦ãƒ‹ãƒƒãƒˆ","æ¤œç´¢ç”¨","ã‚¸ãƒ£ãƒ³ãƒ«","ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—","é…ä¿¡æ—¥","å‹•ç”»ID","ç¢ºåº¦ã‚¹ã‚³ã‚¢"])
        writer.writerows(other_rows)

    rows = singing_rows + other_rows  # çµ±è¨ˆè¡¨ç¤ºç”¨ã«çµåˆ

    safe_print(f"\nå®Œäº†ï¼CSVã‚’å‡ºåŠ›ã—ã¾ã—ãŸ:")
    safe_print(f"   - æ­Œæ : {output_singing} ({len(singing_rows)}ä»¶)")
    safe_print(f"   - ãã®ä»–: {output_other} ({len(other_rows)}ä»¶)")
    safe_print(f"\nçµ±è¨ˆ:")
    safe_print(f"   - å‡¦ç†ã—ãŸå‹•ç”»æ•°: {len(filtered_video_list)}")
    safe_print(f"   - æŠ½å‡ºã—ãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•°: {len(all_timestamps)}")
    safe_print(f"   - æœ€çµ‚å‡ºåŠ›è¡Œæ•°: {len(rows)}")

    if rows:
        # ç¢ºåº¦ã‚¹ã‚³ã‚¢çµ±è¨ˆ
        scores = [float(row[8]) for row in rows]
        high_conf = len([s for s in scores if s > 0.7])
        med_conf = len([s for s in scores if 0.4 <= s <= 0.7])
        low_conf = len([s for s in scores if s < 0.4])

        safe_print(f"\n   ç¢ºåº¦ã‚¹ã‚³ã‚¢åˆ†å¸ƒ:")
        safe_print(f"   - é«˜ç¢ºåº¦ (>0.7): {high_conf}ä»¶ ({high_conf/len(rows)*100:.1f}%)")
        safe_print(f"   - ä¸­ç¢ºåº¦ (0.4-0.7): {med_conf}ä»¶ ({med_conf/len(rows)*100:.1f}%)")
        safe_print(f"   - ä½ç¢ºåº¦ (<0.4): {low_conf}ä»¶ ({low_conf/len(rows)*100:.1f}%)")
        safe_print(f"   - å¹³å‡ç¢ºåº¦: {sum(scores)/len(scores):.2f}")

        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥çµ±è¨ˆ
        genre_stats = {}
        for row in rows:
            genre = row[4]  # ã‚¸ãƒ£ãƒ³ãƒ«åˆ—
            genre_stats[genre] = genre_stats.get(genre, 0) + 1

        safe_print(f"\n   ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥çµ±è¨ˆ:")
        for genre, count in sorted(genre_stats.items(), key=lambda x: x[1], reverse=True):
            safe_print(f"   - {genre}: {count}æ›² ({count/len(rows)*100:.1f}%)")

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰
    vi_dict = [asdict(vi) for vi in filtered_video_list]
    aligned_json_dump(vi_dict, "output/json/comment_info.json")
    safe_print(f"\nãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—JSONã‚‚ä½œæˆ: output/json/comment_info.json")

if __name__ == "__main__":
    main()