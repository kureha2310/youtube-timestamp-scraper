import json
import os
import re
import csv
from datetime import datetime, timezone, timedelta
from dataclasses import asdict
from typing import List, Optional

from googleapiclient import discovery
from dotenv import load_dotenv

# MeCabã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import MeCab
    mecab_reading = MeCab.Tagger('-Oyomi')
    print("MeCab loaded successfully")
except (ImportError, RuntimeError) as e:
    print(f"MeCab not available: {type(e).__name__}. Using simple hiragana conversion.")
    mecab_reading = None

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from utils.infoclass import VideoInfo, CommentInfo, TimeStamp
from utils.utils import aligned_json_dump

load_dotenv()
API_KEY = os.getenv('API_KEY')
if not API_KEY:
    raise RuntimeError("`.env` ã« API_KEY ãŒã‚ã‚Šã¾ã›ã‚“ã€‚YouTube Data API v3 ã®APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

youtube = discovery.build('youtube', 'v3', developerKey=API_KEY)

# å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿
try:
    users = json.load(open('user_ids.json', encoding='utf-8'))
except FileNotFoundError:
    print("user_ids.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
    users = ["UCxxxxxxxxxxxxxxxxxxxxxx"]
    with open('user_ids.json', 'w', encoding='utf-8') as f:
        json.dump(users, f, ensure_ascii=False, indent=2)

class EnhancedAnalyzer:
    def __init__(self):
        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®šç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.vocaloid_keywords = [
            "åˆéŸ³ãƒŸã‚¯","é¡éŸ³ãƒªãƒ³","é¡éŸ³ãƒ¬ãƒ³","å·¡éŸ³ãƒ«ã‚«","MEIKO","KAITO",
            "GUMI","IA","é‡éŸ³ãƒ†ãƒˆ","ã‚¸ãƒŸãƒ¼ã‚µãƒ P","wowaka","ryo","supercell",
            "ã¿ãã¨P","ã‹ã„ã‚Šããƒ™ã‚¢","DECO*27","Neru","40mP","ãƒãƒ«ãƒ¼ãƒ³","n-buna",
            "ãƒ”ãƒã‚­ã‚ªãƒ”ãƒ¼","Chinozo","Orangestar","ã˜ã‚“","ã™ã‚Šãƒ","å…«ç‹å­P","è¶ã€…P",
            "kemu","Kanaria","Omoi","å¤ä»£å­æ˜","ãƒ¡ãƒ«","doriko","ãƒãƒ","EasyPop",
            "Junky","kemu voxx","çŸ³é¢¨å‘‚","ãƒˆãƒ¼ãƒ","ã¬ã‚†ã‚Š","ã‚Œã‚‹ã‚Šã‚Š","femme fatale",
            "ãƒŠãƒã‚¦","nobodyknows","john","Guiano","Dixie Flatline","æ—¥å‘é›»å·¥","æŸŠãƒã‚°ãƒã‚¿ã‚¤ãƒˆ"
        ]
        self.anime_keywords = [
            "æ¶¼å®®ãƒãƒ«ãƒ’","åƒçŸ³æ’«å­","MAHOå ‚","ã©ã†ã¶ã¤ãƒ“ã‚¹ã‚±ãƒƒãƒ„",
            "æ”¾èª²å¾Œãƒ†ã‚£ãƒ¼ã‚¿ã‚¤ãƒ "
        ]
        self.anime_titles = [
            "God knows","æ‹æ„›ã‚µãƒ¼ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³","ã‚·ãƒ«ã‚¨ãƒƒãƒˆ","ãƒ–ãƒ«ãƒ¼ãƒãƒ¼ãƒ‰",
            "ãƒãƒ¬æ™´ã‚Œãƒ¦ã‚«ã‚¤","å›ã®çŸ¥ã‚‰ãªã„ç‰©èª","å‰µä¸–ã®ã‚¢ã‚¯ã‚¨ãƒªã‚ªãƒ³",
            "ã‚ˆã†ã“ãã‚¸ãƒ£ãƒ‘ãƒªãƒ‘ãƒ¼ã‚¯ã¸","ãŠã‚¸ãƒ£é­”å¥³ã‚«ãƒ¼ãƒ‹ãƒãƒ«",
            "ã‚·ãƒ¥ã‚¬ãƒ¼ã‚½ãƒ³ã‚°ã¨ãƒ“ã‚¿ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—","å¤¢ã‚’ã‹ãªãˆã¦ãƒ‰ãƒ©ãˆã‚‚ã‚“",
            "ãƒ«ãƒ¼ã‚¸ãƒ¥ã®ä¼è¨€","ã«ã‚“ã’ã‚“ã£ã¦ã„ã„ãª","å›ã‚’ã®ã›ã¦",
            "ã‚¿ãƒƒãƒ","secret base","ãƒãƒ å¤ªéƒ"
        ]

    def to_hiragana(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã²ã‚‰ãŒãªã«å¤‰æ›"""
        if mecab_reading:
            try:
                reading = mecab_reading.parse(text).strip()
                hiragana = ''
                for char in reading:
                    if 'ã‚¡' <= char <= 'ãƒ¶':
                        hiragana += chr(ord(char) - ord('ã‚¡') + ord('ã'))
                    elif char == 'ãƒµ':
                        hiragana += 'ã‹'
                    elif char == 'ãƒ¶':
                        hiragana += 'ã‘'
                    else:
                        hiragana += char.lower()
                return hiragana
            except:
                pass
        
        # MeCabãŒä½¿ãˆãªã„å ´åˆã®ç°¡æ˜“å¤‰æ›
        return self._simple_katakana_to_hiragana(text.lower())
    
    def _simple_katakana_to_hiragana(self, text: str) -> str:
        """ç°¡æ˜“ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãªå¤‰æ›ï¼ˆè‹±æ•°å­—ãƒ»è¨˜å·ã‚‚å‡¦ç†ï¼‰"""
        result = ''
        for char in text:
            if 'ã‚¡' <= char <= 'ãƒ¶':
                result += chr(ord(char) - ord('ã‚¡') + ord('ã'))
            elif char == 'ãƒµ':
                result += 'ã‹'
            elif char == 'ãƒ¶':
                result += 'ã‘'
            elif 'A' <= char <= 'Z':
                result += char.lower()
            elif char in 'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™':
                # å…¨è§’æ•°å­—ã‚’åŠè§’ã«
                result += str(ord(char) - ord('ï¼'))
            elif char in 'ï¼ˆï¼‰ï¼»ï¼½ï½›ï½':
                # å…¨è§’æ‹¬å¼§ã‚’é™¤å»
                continue
            else:
                result += char
        return result

    def detect_genre(self, title: str, artist: str) -> str:
        """ã‚¸ãƒ£ãƒ³ãƒ«ã‚’è‡ªå‹•åˆ¤å®š"""
        text = f"{title} {artist}"
        if any(k.lower() in text.lower() for k in self.vocaloid_keywords):
            return "Vocaloid"
        if any(k.lower() in text.lower() for k in self.anime_keywords):
            return "ã‚¢ãƒ‹ãƒ¡"
        if any(k.lower() in title.lower() for k in self.anime_titles):
            return "ã‚¢ãƒ‹ãƒ¡"
        return "ãã®ä»–"

    def calculate_confidence_score(self, video_info: VideoInfo) -> float:
        """æ­Œå‹•ç”»ã®ç¢ºåº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ´»ç”¨ï¼‰"""
        title = video_info.title
        description = video_info.description
        
        # æ—¢å­˜ã®is_singing_streamé–¢æ•°ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯
        combined_text = f"{title} {description}".lower()
        singing_keywords = [
            "æ­Œ", "ã†ãŸ", "æ­Œæ ", "ã†ãŸã‚ã", "æ­Œé…ä¿¡", "singing", "sing",
            "ã‚«ãƒ©ã‚ªã‚±", "ã‹ã‚‰ãŠã‘", "karaoke",
            "éŸ³æ¥½", "music", "æ¥½æ›²", "ã‚½ãƒ³ã‚°", "song",
            "ãƒ¡ãƒ‰ãƒ¬ãƒ¼", "medley", "å¼¾ãèªã‚Š",
            "ãƒ©ã‚¤ãƒ–", "live", "æ¼”å¥", "performance",
            "ã‚¢ã‚«ãƒšãƒ©", "acappella", "ã‚³ãƒ¼ãƒ©ã‚¹", "chorus",
            "æ­Œã£ã¦ã¿ãŸ", "ã†ãŸã£ã¦ã¿ãŸ", "æ­Œãƒªãƒ¬ãƒ¼", "æ­Œå›",
            "ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ­Œ", "æ­Œç·´ç¿’", "æ–°æ›²", "cover",
            "ãƒœã‚«ãƒ­", "vocaloid", "ã‚¢ãƒ‹ã‚½ãƒ³", "anime song", "anisong",
            "ã‚»ãƒˆãƒª", "setlist", "ãƒªãƒ", "ãƒªãƒãƒ¼ã‚µãƒ«", "rehearsal"
        ]
        exclude_keywords = [
            "ã‚²ãƒ¼ãƒ ", "game", "gaming", "ãƒ—ãƒ¬ã‚¤", "play",
            "é›‘è«‡", "zatsudan", "talk", "ãŠã—ã‚ƒã¹ã‚Š", "chat",
            "æ–™ç†", "cooking", "ã‚¯ãƒƒã‚­ãƒ³ã‚°", "é£Ÿã¹ã‚‹", "eating",
            "ãŠçµµæã", "çµµ", "drawing", "art", "ã‚¤ãƒ©ã‚¹ãƒˆ",
            "å·¥ä½œ", "craft", "ä½œæ¥­", "work", "study", "å‹‰å¼·"
        ]
        
        singing_score = 0
        for keyword in singing_keywords:
            if keyword in combined_text:
                singing_score += 1
        
        exclude_score = 0
        for keyword in exclude_keywords:
            if keyword in combined_text:
                exclude_score += 1
        
        if re.search(r'[æ­Œã†ãŸã‚¦ã‚¿]', title):
            singing_score += 3
        if re.search(r'[â™ªâ™«â™¬ğŸµğŸ¶ğŸ¤ğŸ¼]', combined_text):
            singing_score += 2
        
        timestamp_count = len(re.findall(r'\d{1,2}:\d{2}', description))
        if timestamp_count >= 3:
            singing_score += 2
        
        # æ­£è¦åŒ–ã—ã¦ã‚¹ã‚³ã‚¢ã‚’0-1ã®ç¯„å›²ã«
        total_possible = len(singing_keywords) + 5 + 2  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•° + ãƒœãƒ¼ãƒŠã‚¹ã‚¹ã‚³ã‚¢
        raw_score = max(0, singing_score - exclude_score)
        return min(1.0, raw_score / 10.0)  # 10ç‚¹æº€ç‚¹ã§æ­£è¦åŒ–

    def clean_title(self, text: str) -> str:
        """å…ˆé ­ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ã‚’é™¤å»"""
        # å…¨è§’æ•°å­—ã‚’åŠè§’ã«çµ±ä¸€
        text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))

        # ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¤‡æ•°å›é©ç”¨ã—ã¦å†å¸°çš„ã«é™¤å»ï¼‰
        # "01. æ›²å" "1) æ›²å" "ã€1ã€‘æ›²å" "(1) æ›²å" ãªã©
        # è¤‡æ•°ã®ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãŒé€£ç¶šã—ã¦ã„ã‚‹å ´åˆã‚‚ã‚ã‚‹ï¼ˆä¾‹: "01. 1) æ›²å"ï¼‰
        max_iterations = 3  # æœ€å¤§3å›ç¹°ã‚Šè¿”ã™

        for _ in range(max_iterations):
            original = text
            patterns = [
                r"^\s*\d{1,3}[\.\ã€‚\)ï¼‰\]ã€‘\-ãƒ¼ãƒ»]\s*",  # "01." "01ã€‚" "1)" "1ã€‘" "1-" "1ãƒ»" ãªã©ï¼ˆå…¨è§’ãƒ”ãƒªã‚ªãƒ‰ã‚‚å«ã‚€ï¼‰
                r"^\s*[\(\(ã€\[]\s*\d{1,3}\s*[\)\)ã€‘\]]\s*",  # "(1)" "ã€1ã€‘" "[1]" ãªã©
                r"^\s*\d{1,3}\s+",  # "01 " (æ•°å­—+ã‚¹ãƒšãƒ¼ã‚¹)
                r"^\s*[ç¬¬]\d{1,3}[æ›²è©±å›ç« ]\s*",  # "ç¬¬1æ›²" "ç¬¬1è©±" ãªã©
            ]

            for pattern in patterns:
                text = re.sub(pattern, "", text)

            # å¤‰åŒ–ãŒãªããªã£ãŸã‚‰çµ‚äº†
            if text == original:
                break

        text = re.sub(r"<br\s*/?>", " ", text, flags=re.IGNORECASE)

        # å…ˆé ­ã®è£…é£¾è¨˜å·ã‚’é™¤å»ï¼ˆ&, ï¼†, â€», â˜…, â˜†, â– , â–¡, â—†, â—‡, â—, â—‹, â–², â–³, â–¼, â–½ãªã©ï¼‰
        text = re.sub(r"^\s*[&ï¼†â€»â˜…â˜†â– â–¡â—†â—‡â—â—‹â–²â–³â–¼â–½â¤â¡â†’â‡’â–ºâ–¶â–ºãƒ»]+\s*", "", text)

        return text.strip()

    def is_valid_song_entry(self, title: str, artist: str) -> bool:
        """æœ‰åŠ¹ãªæ›²ã‚¨ãƒ³ãƒˆãƒªã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåãŒãªã„å ´åˆã¯ç„¡åŠ¹
        if not artist:
            return False

        # æ•°å­—ã¨è¨˜å·ã®ã¿ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ç„¡åŠ¹
        if re.match(r'^[\d\s\.\-\(\)\[\]ã€€]+$', title):
            return False

        # æ›²åãŒçŸ­ã„ï¼ˆ1-2æ–‡å­—ï¼‰å ´åˆã¯ã€æœ‰åŠ¹ãªæ–‡å­—ï¼ˆæ—¥æœ¬èªã€è‹±å­—ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if len(title.strip()) <= 2:
            # æ—¥æœ¬èªï¼ˆæ¼¢å­—ã€ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠï¼‰ã¾ãŸã¯è‹±å­—ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°OK
            if not re.search(r'[a-zA-Zã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾¯]', title):
                return False

        # ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ï¼ˆ"01." "1)" ãªã©ï¼‰ã®å ´åˆã¯ç„¡åŠ¹
        if re.match(r'^\d+[\.\)\-\s]*$', title):
            return False

        # ç„¡åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        invalid_patterns = [
            r'^ã‚»ãƒˆãƒª',
            r'^ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—',
            r'^ãƒªã‚¹ãƒˆ',
            r'^æ›²ç›®',
            r'^\d+æ›²ç›®',
            r'^BGM',
        ]

        for pattern in invalid_patterns:
            if re.search(pattern, title, re.IGNORECASE):
                return False

        return True

    def parse_song_title_artist(self, title: str) -> tuple[str, str]:
        """æ›²åã¨ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã‚’åˆ†é›¢"""
        title = self.clean_title(title)

        # ã€Œæ›² / æ­Œæ‰‹ã€å½¢å¼ã§åˆ†å‰²
        parts = re.split(r"\s*/\s*", title, maxsplit=1)
        if len(parts) == 2:
            # åˆ†å‰²å¾Œã‚‚å„éƒ¨åˆ†ã«å¯¾ã—ã¦clean_titleã‚’é©ç”¨ï¼ˆãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãŒæ›²åå´ã«æ®‹ã£ã¦ã„ã‚‹å ´åˆï¼‰
            song_title = self.clean_title(parts[0].strip())
            artist = parts[1].strip()
            return song_title, artist
        else:
            return title.strip(), ""

def is_singing_stream(title: str, description: str, comments: Optional[List[str]] = None) -> bool:
    """æ­Œå‹•ç”»åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆåˆ†æå¼·åŒ–ç‰ˆï¼‰"""
    combined_text = f"{title} {description}".lower()
    singing_keywords = [
        "æ­Œ", "ã†ãŸ", "æ­Œæ ", "ã†ãŸã‚ã", "æ­Œé…ä¿¡", "singing", "sing",
        "ã‚«ãƒ©ã‚ªã‚±", "ã‹ã‚‰ãŠã‘", "karaoke",
        "éŸ³æ¥½", "music", "æ¥½æ›²", "ã‚½ãƒ³ã‚°", "song",
        "ãƒ¡ãƒ‰ãƒ¬ãƒ¼", "medley", "å¼¾ãèªã‚Š",
        "ãƒ©ã‚¤ãƒ–", "live", "æ¼”å¥", "performance",
        "ã‚¢ã‚«ãƒšãƒ©", "acappella", "ã‚³ãƒ¼ãƒ©ã‚¹", "chorus",
        "æ­Œã£ã¦ã¿ãŸ", "ã†ãŸã£ã¦ã¿ãŸ", "æ­Œãƒªãƒ¬ãƒ¼", "æ­Œå›",
        "ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ­Œ", "æ­Œç·´ç¿’", "æ–°æ›²", "cover",
        "ãƒœã‚«ãƒ­", "vocaloid", "ã‚¢ãƒ‹ã‚½ãƒ³", "anime song", "anisong",
        "ã‚»ãƒˆãƒª", "setlist", "ãƒªãƒ", "ãƒªãƒãƒ¼ã‚µãƒ«", "rehearsal"
    ]
    exclude_keywords = [
        "ã‚²ãƒ¼ãƒ ", "game", "gaming", "ãƒ—ãƒ¬ã‚¤", "play",
        "é›‘è«‡", "zatsudan", "talk", "ãŠã—ã‚ƒã¹ã‚Š", "chat",
        "æ–™ç†", "cooking", "ã‚¯ãƒƒã‚­ãƒ³ã‚°", "é£Ÿã¹ã‚‹", "eating",
        "ãŠçµµæã", "çµµ", "drawing", "art", "ã‚¤ãƒ©ã‚¹ãƒˆ",
        "å·¥ä½œ", "craft", "ä½œæ¥­", "work", "study", "å‹‰å¼·"
    ]
    singing_score = 0
    for keyword in singing_keywords:
        if keyword in combined_text:
            singing_score += 1
    exclude_score = 0
    for keyword in exclude_keywords:
        if keyword in combined_text:
            exclude_score += 1
    if re.search(r'[æ­Œã†ãŸã‚¦ã‚¿]', title):
        singing_score += 3
    if re.search(r'[â™ªâ™«â™¬ğŸµğŸ¶ğŸ¤ğŸ¼]', combined_text):
        singing_score += 2
    timestamp_count = len(re.findall(r'\d{1,2}:\d{2}', description))
    if timestamp_count >= 3:
        singing_score += 2

    # ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã«ã‚ˆã‚‹è¿½åŠ ã‚¹ã‚³ã‚¢
    if comments:
        comment_timestamp_count = 0
        for comment in comments:
            comment_timestamps = len(re.findall(r'\d{1,2}:\d{2}', comment))
            if comment_timestamps >= 3:  # 1ã‚³ãƒ¡ãƒ³ãƒˆã«3ã¤ä»¥ä¸Šã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                comment_timestamp_count += 1

        # ã‚³ãƒ¡ãƒ³ãƒˆã«å¤šæ•°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒã‚ã‚‹å ´åˆã€æ­Œé…ä¿¡ã®å¯èƒ½æ€§ãŒé«˜ã„
        if comment_timestamp_count >= 2:
            singing_score += 4
        elif comment_timestamp_count >= 1:
            singing_score += 2

    if singing_score >= 2 and exclude_score <= singing_score:
        return True
    elif singing_score >= 4:
        return True
    else:
        return False

def get_uploads_playlist_id(channel_id: str) -> str | None:
    """æ—¢å­˜é–¢æ•°ã‚’ãã®ã¾ã¾ä½¿ç”¨"""
    if not channel_id or not channel_id.startswith("UC"):
        return None
    try:
        resp = youtube.channels().list(
            part="contentDetails",
            id=channel_id,
            fields="items/contentDetails/relatedPlaylists/uploads"
        ).execute()
        items = resp.get("items", [])
        if not items:
            return None
        return items[0]["contentDetails"]["relatedPlaylists"]["uploads"]
    except Exception as e:
        print(f"ãƒãƒ£ãƒ³ãƒãƒ« {channel_id} ã® uploads ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_video_info_in_playlist(playlist_id: str) -> list[VideoInfo]:
    """æ—¢å­˜é–¢æ•°ã‚’ãã®ã¾ã¾ä½¿ç”¨"""
    video_info_list: list[VideoInfo] = []
    try:
        request = youtube.playlistItems().list(
            part="snippet",
            playlistId=playlist_id,
            maxResults=50,
            fields="nextPageToken,items/snippet(publishedAt,title,description,resourceId/videoId)"
        )
        while request:
            response = request.execute()
            items = response.get("items", [])
            for i in items:
                vi = VideoInfo.from_response_snippet(i["snippet"])
                vid = vi.id

                # --- å‹•ç”»è©³ç´°ã‚’è¿½åŠ ã§å–å¾— ---
                try:
                    details = youtube.videos().list(
                        part="liveStreamingDetails,snippet",
                        id=vid,
                        fields="items(snippet/publishedAt,liveStreamingDetails/actualStartTime)"
                    ).execute()

                    if details.get("items"):
                        item = details["items"][0]
                        vi.stream_start = item.get("liveStreamingDetails", {}).get("actualStartTime")
                        if not vi.stream_start:
                            vi.stream_start = item["snippet"]["publishedAt"]

                except Exception as e:
                    print(f"å‹•ç”» {vid} ã®è©³ç´°å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")

                video_info_list.append(vi)

            request = youtube.playlistItems().list_next(request, response)
    except Exception as e:
        print(f"ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ {playlist_id} ã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
    return video_info_list

def get_comments(video_id: str) -> list[CommentInfo]:
    """æ—¢å­˜é–¢æ•°ã‚’ãã®ã¾ã¾ä½¿ç”¨"""
    comment_list: list[CommentInfo] = []
    comment_field = "snippet(videoId,textDisplay,textOriginal)"
    top_comment_f = f"items/snippet/topLevelComment/{comment_field}"
    replies_f = f"items/replies/comments/{comment_field}"

    try:
        request = youtube.commentThreads().list(
            part="snippet,replies",
            maxResults=100,
            videoId=video_id,
            fields=f"nextPageToken,{top_comment_f},{replies_f}"
        )
        while request:
            response = request.execute()
            for item in response.get("items", []):
                comment_list.extend(CommentInfo.response_item_to_comments(item))
            request = youtube.commentThreads().list_next(request, response)
    except Exception as e:
        print(f"å‹•ç”» {video_id} ã®ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")

    return comment_list

def main():
    print("YouTubeæ­Œå‹•ç”»ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡ºãƒ„ãƒ¼ãƒ«ï¼ˆçµ±åˆç‰ˆï¼‰")
    print("=" * 60)
    
    analyzer = EnhancedAnalyzer()
    
    # 1. å‹•ç”»æƒ…å ±å–å¾—ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    uploads_ids: list[str] = []
    for uc in users:
        up = get_uploads_playlist_id(uc)
        if up:
            uploads_ids.append(up)
        else:
            print(f"å–å¾—å¤±æ•—: {uc}")

    video_info_list: list[VideoInfo] = []
    for upid in uploads_ids:
        video_info_list += get_video_info_in_playlist(upid)

    # 2. æ­Œå‹•ç”»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆå–å¾—å‰ã«ä¸€æ¬¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
    filtered_video_list = []
    for vi in video_info_list:
        if is_singing_stream(vi.title, vi.description):
            filtered_video_list.append(vi)

    print(f"å…¨å‹•ç”»æ•°: {len(video_info_list)}, æ­Œæ å‹•ç”»æ•°: {len(filtered_video_list)}")

    print("\n=== æ­Œæ ã¨ã—ã¦æ¤œå‡ºã•ã‚ŒãŸå‹•ç”» ===")
    for i, vi in enumerate(filtered_video_list[:10]):
        try:
            print(f"{i+1}. {vi.title}")
        except UnicodeEncodeError:
            # çµµæ–‡å­—ãªã©ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤º
            safe_title = vi.title.encode('ascii', 'ignore').decode('ascii')
            print(f"{i+1}. {safe_title} [...]")
    if len(filtered_video_list) > 10:
        print(f"... ä»– {len(filtered_video_list) - 10} ä»¶")

    # 3. ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— + å†ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    print("\nã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ä¸­...")
    secondary_filtered_list = []
    for i, video_info in enumerate(filtered_video_list):
        try:
            print(f"{i+1}/{len(filtered_video_list)}: {video_info.title}")
        except UnicodeEncodeError:
            print(f"{i+1}/{len(filtered_video_list)}: [title with emoji]")
        video_info.comments = get_comments(video_info.id)

        # ã‚³ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’å«ã‚ã¦å†åˆ¤å®š
        comment_texts = [c.text_display for c in video_info.comments] if video_info.comments else []
        if is_singing_stream(video_info.title, video_info.description, comment_texts):
            secondary_filtered_list.append(video_info)
        else:
            print(f"  â†’ ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã«ã‚ˆã‚Šé™¤å¤–")

    filtered_video_list = secondary_filtered_list
    print(f"\nã‚³ãƒ¡ãƒ³ãƒˆåˆ†æå¾Œã®æ­Œæ å‹•ç”»æ•°: {len(filtered_video_list)}")

    # 4. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º
    print("\nã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡ºä¸­...")
    all_timestamps = []
    for v in filtered_video_list:
        ts_list = TimeStamp.from_videoinfo(v)
        all_timestamps.extend(ts_list)
    
    print(f"æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•°: {len(all_timestamps)}")

    # 5. CSVå½¢å¼ã«å¤‰æ›ï¼ˆé‡è¤‡é™¤å»å¼·åŒ–ç‰ˆï¼‰
    print("\nCSVå½¢å¼ã«å¤‰æ›ä¸­...")
    rows = []
    seen = {}
    duplicate_groups = {}  # é‡è¤‡ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    idx = 1

    # ç¬¬1ãƒ‘ã‚¹: ã™ã¹ã¦ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    for entry in all_timestamps:
        video_id = entry.video_id
        raw_title = entry.text
        timestamp = entry.timestamp
        published_at = getattr(entry, 'stream_start', None) or entry.published_at

        # ç¢ºåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆè©²å½“ã™ã‚‹å‹•ç”»ã‚’è¦‹ã¤ã‘ã¦è¨ˆç®—ï¼‰
        confidence = 0.0
        for vi in filtered_video_list:
            if vi.id == video_id:
                confidence = analyzer.calculate_confidence_score(vi)
                break

        song_title, artist = analyzer.parse_song_title_artist(raw_title)

        # ç„¡åŠ¹ãªã‚¨ãƒ³ãƒˆãƒªã¯é™¤å¤–ï¼ˆæ­Œæ‰‹ãªã—ã€ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ã®ã¿ã€ãªã©ï¼‰
        if not analyzer.is_valid_song_entry(song_title, artist):
            continue

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç§’ã«å¤‰æ›ï¼ˆÂ±5ç§’ä»¥å†…ã¯åŒã˜ã¨ã¿ãªã™ï¼‰
        time_parts = timestamp.split(':')
        total_seconds = 0
        try:
            if len(time_parts) == 2:  # mm:ss
                total_seconds = int(time_parts[0]) * 60 + int(time_parts[1])
            elif len(time_parts) == 3:  # hh:mm:ss
                total_seconds = int(time_parts[0]) * 3600 + int(time_parts[1]) * 60 + int(time_parts[2])
        except:
            total_seconds = 0

        # æ­£è¦åŒ–ã‚­ãƒ¼ï¼ˆæ›²åã¨ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã®é¡ä¼¼æ€§ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®è¿‘ã•ã§åˆ¤å®šï¼‰
        normalized_key = (
            song_title.lower().strip(),
            artist.lower().strip(),
            video_id,
            total_seconds // 5  # 5ç§’å˜ä½ã§ä¸¸ã‚ã‚‹
        )

        if normalized_key not in duplicate_groups:
            duplicate_groups[normalized_key] = []

        duplicate_groups[normalized_key].append({
            'raw_title': raw_title,
            'song_title': song_title,
            'artist': artist,
            'timestamp': timestamp,
            'total_seconds': total_seconds,
            'video_id': video_id,
            'published_at': published_at,
            'confidence': confidence,
            'has_numbering': bool(re.match(r"^\s*\d+", raw_title))
        })

    # ç¬¬2ãƒ‘ã‚¹: å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æœ€é©ãªã‚‚ã®ã‚’é¸æŠ
    for normalized_key, duplicates in duplicate_groups.items():
        # å„ªå…ˆé †ä½: ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãªã— > è©³ç´°ãªæ›²å > é•·ã„æ›²å
        best = max(duplicates, key=lambda x: (
            not x['has_numbering'],  # ãƒŠãƒ³ãƒãƒªãƒ³ã‚°ãŒãªã„æ–¹ãŒå„ªå…ˆ
            len(x['song_title']),     # æ›²åãŒé•·ã„æ–¹ãŒè©³ç´°
            len(x['artist'])          # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåãŒé•·ã„æ–¹ãŒè©³ç´°
        ))

        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®š
        genre = analyzer.detect_genre(best['song_title'], best['artist'])

        # ã²ã‚‰ãŒãªå¤‰æ›
        search_text = analyzer.to_hiragana(best['song_title'])

        # æ—¥ä»˜ã‚’JSTã¸
        try:
            dt = datetime.fromisoformat((best['published_at'] or "").replace("Z", "+00:00"))
            date_str = dt.astimezone(timezone(timedelta(hours=9))).strftime("%Y/%m/%d")
        except Exception:
            date_str = ""

        rows.append([
            idx,
            best['song_title'],
            best['artist'],
            search_text,
            genre,
            best['timestamp'],
            date_str,
            best['video_id'],
            f"{best['confidence']:.2f}",
            best['total_seconds']  # ã‚½ãƒ¼ãƒˆç”¨ã«è¿½åŠ ï¼ˆCSVå‡ºåŠ›æ™‚ã«ã¯é™¤å¤–ï¼‰
        ])
        idx += 1

    # é…ä¿¡æ—¥ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰
    rows.sort(key=lambda x: (x[6], x[9]))  # é…ä¿¡æ—¥ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆç§’ï¼‰ã§ã‚½ãƒ¼ãƒˆ

    # Noåˆ—ã‚’æŒ¯ã‚Šç›´ã—ã€ã‚½ãƒ¼ãƒˆç”¨ã®åˆ—ã‚’å‰Šé™¤
    for i, row in enumerate(rows, 1):
        row[0] = i
        row.pop()  # total_secondsã‚’å‰Šé™¤

    # 6. CSVå‡ºåŠ›
    output_file = "song_timestamps_complete.csv"
    with open(output_file, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["No","æ›²","æ­Œæ‰‹-ãƒ¦ãƒ‹ãƒƒãƒˆ","æ¤œç´¢ç”¨","ã‚¸ãƒ£ãƒ³ãƒ«","ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—","é…ä¿¡æ—¥","å‹•ç”»ID","ç¢ºåº¦ã‚¹ã‚³ã‚¢"])
        writer.writerows(rows)

    print(f"\nå®Œäº†ï¼CSVã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {output_file}")
    print(f"çµ±è¨ˆ:")
    print(f"   - å‡¦ç†ã—ãŸå‹•ç”»æ•°: {len(filtered_video_list)}")
    print(f"   - æŠ½å‡ºã—ãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•°: {len(all_timestamps)}")
    print(f"   - æœ€çµ‚å‡ºåŠ›è¡Œæ•°: {len(rows)}")

    # ç¢ºåº¦ã‚¹ã‚³ã‚¢çµ±è¨ˆ
    if rows:
        scores = [float(row[8]) for row in rows]
        high_conf = len([s for s in scores if s > 0.7])
        med_conf = len([s for s in scores if 0.4 <= s <= 0.7])
        low_conf = len([s for s in scores if s < 0.4])

        print(f"   - é«˜ç¢ºåº¦ (>0.7): {high_conf}ä»¶")
        print(f"   - ä¸­ç¢ºåº¦ (0.4-0.7): {med_conf}ä»¶")
        print(f"   - ä½ç¢ºåº¦ (<0.4): {low_conf}ä»¶")

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰
    vi_dict = [asdict(vi) for vi in filtered_video_list]
    aligned_json_dump(vi_dict, "comment_info.json")
    print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—JSONã‚‚ä½œæˆ: comment_info.json")

if __name__ == "__main__":
    main()